{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FLACArray","text":"<p>This package provides a set of tools for compressing multi-dimensional arrays where the last array dimension consists of \"streams\" of data. These streams are compressed with the FLAC algorithm and can be written to different file formats as well as decompressed back into numpy arrays.</p> <p>FLAC compression is particularly suited to \"noisy\" timestreams that do not compress well with DEFLATE algorithms used by zip / gzip. This type of data is found in audio signals, scientific timestreams, etc.</p> <p>In the <code>flacarray</code> package we use only a small subset of features found in the libFLAC library. In particular, each data stream is compressed as either one or two 32 bit \"channels\". Stream data consisting of 32 or 64 bit integers are compressed in a loss-less fashion. Floating point data is converted to either 32 or 64 bit integers with a user-specified precision or quantization.</p> <p>If you are specifically working with audio data and want to write flac format audio files, you should look at other software tools such as pyflac.</p>"},{"location":"cookbook/","title":"Cook Book","text":"In\u00a0[1]: Copied! <pre># Set the number of OpenMP threads to use\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\n</pre> # Set the number of OpenMP threads to use import os os.environ[\"OMP_NUM_THREADS\"] = \"4\" In\u00a0[2]: Copied! <pre>import time\nimport numpy as np\nimport h5py\nfrom flacarray import FlacArray, demo\nimport flacarray.hdf5\n</pre> import time import numpy as np import h5py from flacarray import FlacArray, demo import flacarray.hdf5 In\u00a0[3]: Copied! <pre># Create a 2D array of streams\narr, _ = demo.create_fake_data((1000, 100000), dtype=np.float32)\n# How large is this in memory?\nprint(f\"Input array is {arr.nbytes} bytes\")\n</pre> # Create a 2D array of streams arr, _ = demo.create_fake_data((1000, 100000), dtype=np.float32) # How large is this in memory? print(f\"Input array is {arr.nbytes} bytes\") <pre>Input array is 400000000 bytes\n</pre> In\u00a0[4]: Copied! <pre># Compress this with threads\nstart = time.perf_counter()\n\nflcarr = FlacArray.from_array(arr, quanta=1.0e-7, use_threads=True)\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\n</pre> # Compress this with threads start = time.perf_counter()  flcarr = FlacArray.from_array(arr, quanta=1.0e-7, use_threads=True)  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") <pre>Elapsed = 1.23 seconds\n</pre> In\u00a0[5]: Copied! <pre># Compress this without threads\nstart = time.perf_counter()\n\nflcarr = FlacArray.from_array(arr, quanta=1.0e-7, use_threads=False)\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\n</pre> # Compress this without threads start = time.perf_counter()  flcarr = FlacArray.from_array(arr, quanta=1.0e-7, use_threads=False)  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") <pre>Elapsed = 2.6 seconds\n</pre> In\u00a0[6]: Copied! <pre># Decompress the whole thing\nstart = time.perf_counter()\n\nrestored = flcarr.to_array()\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\n</pre> # Decompress the whole thing start = time.perf_counter()  restored = flcarr.to_array()  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") <pre>Elapsed = 0.447 seconds\n</pre> In\u00a0[7]: Copied! <pre># Decompress the whole thing with threads\ndel restored\nstart = time.perf_counter()\n\nrestored = flcarr.to_array(use_threads=True)\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\n</pre> # Decompress the whole thing with threads del restored start = time.perf_counter()  restored = flcarr.to_array(use_threads=True)  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") <pre>Elapsed = 0.439 seconds\n</pre> In\u00a0[8]: Copied! <pre>n_end = 10000\nstart = time.perf_counter()\n\nend_arr = flcarr.to_array(stream_slice=slice(-n_end, None, 1))\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\n</pre> n_end = 10000 start = time.perf_counter()  end_arr = flcarr.to_array(stream_slice=slice(-n_end, None, 1))  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") <pre>Elapsed = 0.43 seconds\n</pre> In\u00a0[9]: Copied! <pre>n_end = 10000\nkeep = np.zeros(arr.shape[:-1], dtype=bool)\nkeep[500] = True\nstart = time.perf_counter()\n\nsub_arr = flcarr.to_array(keep=keep, stream_slice=slice(-n_end, None, 1))\n\nstop = time.perf_counter()\nprint(f\"Elapsed = {stop-start:0.3} seconds\")\nprint(sub_arr)\n</pre> n_end = 10000 keep = np.zeros(arr.shape[:-1], dtype=bool) keep[500] = True start = time.perf_counter()  sub_arr = flcarr.to_array(keep=keep, stream_slice=slice(-n_end, None, 1))  stop = time.perf_counter() print(f\"Elapsed = {stop-start:0.3} seconds\") print(sub_arr) <pre>Elapsed = 0.00201 seconds\n[[ 1.3499217   1.1607051  -1.0080613  ...  0.2447555   1.0821551\n   0.03497732]]\n</pre> <p>So, we can see that decompressing a small number of random samples from a multi-GB dataset in memory is very fast.</p> In\u00a0[10]: Copied! <pre># Create a single timestream\narr, _ = demo.create_fake_data((10000,), dtype=np.float32)\n# How large is this in memory?\nprint(f\"Input array is {arr.nbytes} bytes\")\n</pre> # Create a single timestream arr, _ = demo.create_fake_data((10000,), dtype=np.float32) # How large is this in memory? print(f\"Input array is {arr.nbytes} bytes\") <pre>Input array is 40000 bytes\n</pre> In\u00a0[11]: Copied! <pre># Plot this\ndemo.plot_data(arr)\n</pre> # Plot this demo.plot_data(arr) <p>Now compress / decompress this data with a very conservative <code>quanta</code> value:</p> In\u00a0[12]: Copied! <pre># Create a compressed array\nflcarr = FlacArray.from_array(arr, quanta=1.0e-8)\n</pre> # Create a compressed array flcarr = FlacArray.from_array(arr, quanta=1.0e-8) <p>How big is this compressed array?</p> In\u00a0[13]: Copied! <pre>print(f\"FlacArray is {flcarr.nbytes} bytes\")\n</pre> print(f\"FlacArray is {flcarr.nbytes} bytes\") <pre>FlacArray is 36161 bytes\n</pre> <p>So we see that our compression factor is only about 0.9 (not good)</p> In\u00a0[14]: Copied! <pre># Restore back to an array\nrestored = flcarr.to_array()\n</pre> # Restore back to an array restored = flcarr.to_array() In\u00a0[15]: Copied! <pre>demo.plot_data(restored)\n</pre> demo.plot_data(restored) In\u00a0[16]: Copied! <pre># Difference\nresidual = restored - arr\n</pre> # Difference residual = restored - arr In\u00a0[17]: Copied! <pre>demo.plot_data(residual)\n</pre> demo.plot_data(residual) <p>The residual difference after the roundtrip compression / decompression is close to the machine precision for float32.</p> In\u00a0[18]: Copied! <pre># Create a compressed array\nflcarr = FlacArray.from_array(arr, quanta=1.0e-4)\n</pre> # Create a compressed array flcarr = FlacArray.from_array(arr, quanta=1.0e-4) <p>How big is this lower-precision compressed array?</p> In\u00a0[19]: Copied! <pre>print(f\"FlacArray is {flcarr.nbytes} bytes\")\n</pre> print(f\"FlacArray is {flcarr.nbytes} bytes\") <pre>FlacArray is 19664 bytes\n</pre> <p>So we have used information about our data to avoid storing unnecessary precision and have improved our compression ratio.</p> In\u00a0[20]: Copied! <pre># Restore back to an array\nrestored = flcarr.to_array()\n</pre> # Restore back to an array restored = flcarr.to_array() In\u00a0[21]: Copied! <pre>demo.plot_data(restored)\n</pre> demo.plot_data(restored) In\u00a0[22]: Copied! <pre># Difference\nresidual = restored - arr\n</pre> # Difference residual = restored - arr In\u00a0[23]: Copied! <pre>demo.plot_data(residual)\n</pre> demo.plot_data(residual) <p>As expected, the residual is now comparable to the size of the quanta that we used.</p>"},{"location":"cookbook/#cook-book","title":"Cook Book\u00b6","text":"<p>This notebook contains some more advanced examples addressing common usage patterns.  Look at the Tutorial first to get a better sense of the big picture of the tools.</p>"},{"location":"cookbook/#random-access-to-large-arrays","title":"Random Access to Large Arrays\u00b6","text":"<p>Consider a common case where we have a 2D array that represents essentially a \"list\" of timestreams of data.  We might have thousands of timestreams, each with millions of samples.  Now we want to decompress and access a subset of those streams and / or samples.  To reduce memory in this notebook we are using a slightly smaller array.</p>"},{"location":"cookbook/#subset-of-samples-for-all-streams","title":"Subset of Samples for All Streams\u00b6","text":"<p>If our 2D array of streams contains co-sampled data, we might mant to examine a slice in time of all streams.  Imagine we wanted to get data near the end of the array for all streams:</p>"},{"location":"cookbook/#subset-of-samples-for-a-few-streams","title":"Subset of Samples for a Few Streams\u00b6","text":"<p>Imagine we want the last 1000 samples of one stream in the middle.  We can use a \"keep\" mask combined with a sample slice:</p>"},{"location":"cookbook/#quantization-effects","title":"Quantization Effects\u00b6","text":"<p>As mentioned previously, 32 and 64 bit integer data is compressed in a lossless fashion with FLAC, using either one or two audio channels.  When compressing 32 or 64 bit floating point data, a choice must be made about the \"amount\" of floating point value assigned to each integer.  This <code>quanta</code> parameter is a tradeoff between fidelity of the input data and compression factor.  One \"safe\" choice is to pick a quantization value that is near the machine epsilon for float32 or float64.  Although this will ensure nearly lossless compression, the compression ratio will be very poor.</p> <p>To achieve better compression of floating point data, you should consider the dynamic range and actual precision of this data.  As an example, consider some fake 32 bit floating point data:</p>"},{"location":"cookbook/#decreased-precision","title":"Decreased Precision\u00b6","text":"<p>Now imagine that we know the underlying precision of our data above is not really at the level of machine precision for float32.  Instead, we know that our data came from measurements with a precision of 1.0e-4 in the units of this dataset.  We can use that information when compressing:</p>"},{"location":"cookbook/#parallel-io","title":"Parallel I/O\u00b6","text":"<p>To-Do:  Discuss</p> <ul> <li>Interaction of threads, OpenMP versus libFLAC pthreads</li> <li>Use of MPI HDF5 with h5py</li> </ul>"},{"location":"dev/","title":"Developer Notes","text":"<p>To-Do</p> <p>Discuss: - Code formatting (ruff) - PR workflow</p>"},{"location":"install/","title":"Installation","text":"<p>For most use cases, you can just install <code>flacarray</code> from pre-built python wheels or conda packages. For specialized use cases or development it is straightforward to build the package from source using either a conda environment for dependencies or with those obtained through your OS package manager.</p>"},{"location":"install/#python-wheels","title":"Python Wheels","text":"<p>You can install pre-built wheels from PyPI using pip within a virtualenv:</p> <pre><code>pip install flacarray\n</code></pre> <p>Or, if you are using a shared python environment you can install to a user location with:</p> <pre><code>pip install --user flacarray\n</code></pre>"},{"location":"install/#conda-packages","title":"Conda Packages","text":"<p>If you are using a conda environment you can install the conda package for <code>flacarray</code> from the conda-forge channels:</p> <pre><code>conda install -c conda-forge flacarray\n</code></pre> <p>To-Do</p> <p><code>flacarray</code> is not yet on conda-forge</p>"},{"location":"install/#building-from-source","title":"Building From Source","text":"<p>In order to build from source, you will need a C compiler and the FLAC development libraries installed.</p>"},{"location":"install/#building-within-a-conda-environment","title":"Building Within a Conda Environment","text":"<p>If you have conda available, you can create an environment will all the dependencies you need to build flacarray from source. For this example, we create an environment called \"flacarray\". First create the env with all dependencies and activate it (FIXME: add a requirements file for dev):</p> <pre><code>conda create -n flacarray \\\n    c-compiler numpy libflac cython meson-python pkgconfig\n\nconda activate flacarray\n</code></pre> <p>Now you can go into your local git checkout of the flacarray source and do:</p> <pre><code>pip install .\n</code></pre> <p>To build and install the package.</p> <p>To also work on docs, install additional packages:</p> <pre><code>conda install mkdocs mkdocstrings mkdocstrings-python mkdocs-jupyter\npip install mkdocs-print-site-plugin\n</code></pre>"},{"location":"install/#other-ways-of-building","title":"Other Ways of Building","text":"<p>To-Do</p> <p>Discuss OS packages, document apt, rpm, homebrew options.</p>"},{"location":"reference/","title":"API Reference","text":"<p>The <code>flacarray</code> package consists of a primary class (<code>FlacArray</code>) plus a variety of helper functions.</p>"},{"location":"reference/#compressed-array-representation","title":"Compressed Array Representation","text":"<p>The <code>FlacArray</code> class stores a compressed representation of an N dimensional array where the last dimension consists of \"streams\" of numbers to be compressed.</p>"},{"location":"reference/#flacarray.FlacArray","title":"<code>flacarray.FlacArray</code>","text":"<p>FLAC compressed array representation.</p> <p>This class holds a compressed representation of an N-dimensional array.  The final (fastest changing) dimension is the axis along which the data is compressed.  Each of the vectors in this last dimension is called a \"stream\" here.  The leading dimensions of the original matrix form an array of these streams.</p> <p>Internally, the data is stored as a contiguous concatenation of the bytes from these compressed streams.  A separate array contains the starting byte of each stream in the overall bytes array.  The shape of the starting array corresponds to the shape of the leading, un-compressed dimensions of the original array.</p> <p>If the input data is 32bit or 64bit integers, each stream in the array is compressed directly with FLAC.</p> <p>If the input data is 32bit or 64bit floating point numbers, then you must specify exactly one of either quanta or precision when calling <code>from_array()</code>.  For floating point data, the mean of each stream is computed and rounded to the nearest whole quanta.  This \"offset\" per stream is recorded and subtracted from the stream.  The offset-subtracted stream data is then rescaled and truncated to integers (int32 or int64 depending on the bit width of the input array).  If <code>quanta</code> is specified, the data is rescaled by 1 / quanta.  The quanta may either be a scalar applied to all streams, or an array of values, one per stream.  If instead the precision (integer number of decimal places) is specified, this is converted to a quanta by dividing the stream RMS by <code>10^{precision}</code>.  Similar to quanta, the precision may be specified as a single value for all streams, or as an array of values, one per stream.</p> <p>If you choose a quanta value that is close to machine epsilon (e.g. 1e-7 for 32bit or 1e-16 for 64bit), then the compression amount will be negligible but the results nearly lossless. Compression of floating point data should not be done blindly and you should consider the underlying precision of the data you are working with in order to achieve the best compression possible.</p> <p>The following rules summarize the data conversion that is performed depending on the input type:</p> <ul> <li> <p>int32:  No conversion.  Compressed to single channel FLAC bytestream.</p> </li> <li> <p>int64:  No conversion.  Compressed to 2-channel (stereo) FLAC bytestream.</p> </li> <li> <p>float32:  Subtract the offset per stream and scale data based on the quanta value     or precision (see above).  Then round to nearest 32bit integer.</p> </li> <li> <p>float64:  Subtract the offset per stream and scale data based on the quanta value     or precision (see above).  Then round to nearest 64bit integer.</p> </li> </ul> <p>After conversion to integers, each stream's data is separately compressed into a sequence of FLAC bytes, which is appended to the bytestream.  The offset in bytes for each stream is recorded.</p> <p>A FlacArray is only constructed directly when making a copy.  Use the class methods to create FlacArrays from numpy arrays or on-disk representations.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>FlacArray</code> <p>Construct a copy of the input FlacArray.</p> required Source code in <code>flacarray/array.py</code> <pre><code>class FlacArray:\n    \"\"\"FLAC compressed array representation.\n\n    This class holds a compressed representation of an N-dimensional array.  The final\n    (fastest changing) dimension is the axis along which the data is compressed.  Each\n    of the vectors in this last dimension is called a \"stream\" here.  The leading\n    dimensions of the original matrix form an array of these streams.\n\n    Internally, the data is stored as a contiguous concatenation of the bytes from\n    these compressed streams.  A separate array contains the starting byte of each\n    stream in the overall bytes array.  The shape of the starting array corresponds\n    to the shape of the leading, un-compressed dimensions of the original array.\n\n    If the input data is 32bit or 64bit integers, each stream in the array is\n    compressed directly with FLAC.\n\n    If the input data is 32bit or 64bit floating point numbers, then you **must**\n    specify exactly one of either quanta or precision when calling `from_array()`.  For\n    floating point data, the mean of each stream is computed and rounded to the nearest\n    whole quanta.  This \"offset\" per stream is recorded and subtracted from the\n    stream.  The offset-subtracted stream data is then rescaled and truncated to\n    integers (int32 or int64 depending on the bit width of the input array).  If\n    `quanta` is specified, the data is rescaled by 1 / quanta.  The quanta may either\n    be a scalar applied to all streams, or an array of values, one per stream.  If\n    instead the precision (integer number of decimal places) is specified, this is\n    converted to a quanta by dividing the stream RMS by `10^{precision}`.  Similar to\n    quanta, the precision may be specified as a single value for all streams, or as an\n    array of values, one per stream.\n\n    If you choose a quanta value that is close to machine epsilon (e.g. 1e-7 for 32bit\n    or 1e-16 for 64bit), then the compression amount will be negligible but the results\n    nearly lossless. Compression of floating point data should not be done blindly and\n    you should consider the underlying precision of the data you are working with in\n    order to achieve the best compression possible.\n\n    The following rules summarize the data conversion that is performed depending on\n    the input type:\n\n    * int32:  No conversion.  Compressed to single channel FLAC bytestream.\n\n    * int64:  No conversion.  Compressed to 2-channel (stereo) FLAC bytestream.\n\n    * float32:  Subtract the offset per stream and scale data based on the quanta value\n        or precision (see above).  Then round to nearest 32bit integer.\n\n    * float64:  Subtract the offset per stream and scale data based on the quanta value\n        or precision (see above).  Then round to nearest 64bit integer.\n\n    After conversion to integers, each stream's data is separately compressed into a\n    sequence of FLAC bytes, which is appended to the bytestream.  The offset in bytes\n    for each stream is recorded.\n\n    A FlacArray is only constructed directly when making a copy.  Use the class methods\n    to create FlacArrays from numpy arrays or on-disk representations.\n\n    Args:\n        other (FlacArray):  Construct a copy of the input FlacArray.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        other,\n        shape=None,\n        global_shape=None,\n        compressed=None,\n        dtype=None,\n        stream_starts=None,\n        stream_nbytes=None,\n        stream_offsets=None,\n        stream_gains=None,\n        mpi_comm=None,\n        mpi_dist=None,\n    ):\n        if other is not None:\n            # We are copying an existing object, make sure we have an\n            # independent copy.\n            self._shape = copy.deepcopy(other._shape)\n            self._global_shape = copy.deepcopy(other._global_shape)\n            self._compressed = copy.deepcopy(other._compressed)\n            self._dtype = np.dtype(other._dtype)\n            self._stream_starts = copy.deepcopy(other._stream_starts)\n            self._stream_nbytes = copy.deepcopy(other._stream_nbytes)\n            self._stream_offsets = copy.deepcopy(other._stream_offsets)\n            self._stream_gains = copy.deepcopy(other._stream_gains)\n            self._mpi_dist = copy.deepcopy(other._mpi_dist)\n            # MPI communicators can be limited in number and expensive to create.\n            self._mpi_comm = other._mpi_comm\n        else:\n            # This form of constructor is used in the class methods where we\n            # have already created these arrays for use by this instance.\n            self._shape = shape\n            self._global_shape = global_shape\n            self._compressed = compressed\n            self._dtype = np.dtype(dtype)\n            self._stream_starts = stream_starts\n            self._stream_nbytes = stream_nbytes\n            self._stream_offsets = stream_offsets\n            self._stream_gains = stream_gains\n            self._mpi_comm = mpi_comm\n            self._mpi_dist = mpi_dist\n        self._init_params()\n\n    def _init_params(self):\n        # The input `_shape` parameter is the original shape when the instance\n        # was created from an array or read from disk.  In the case of a single\n        # stream, this tracks the user intentions about whether to flatten the\n        # leading dimension.  We also track the \"local shape\", with is the same,\n        # but which always keeps the leading dimension.\n        if len(self._shape) == 1:\n            self._flatten_single = True\n            self._local_shape = (1, self._shape[0])\n        else:\n            self._flatten_single = False\n            self._local_shape = self._shape\n\n        self._local_nbytes = self._compressed.nbytes\n        (\n            self._global_nbytes,\n            self._global_proc_nbytes,\n            self._global_stream_starts,\n        ) = global_bytes(self._local_nbytes, self._stream_starts, self._mpi_comm)\n        self._leading_shape = self._local_shape[:-1]\n        self._global_leading_shape = self._global_shape[:-1]\n        self._stream_size = self._local_shape[-1]\n\n        # For reference, record the type string of the original data.\n        self._typestr = self._dtype_str(self._dtype)\n        # Track whether we have 32bit or 64bit data\n        self._is_int64 = self._dtype == np.dtype(np.int64) or self._dtype == np.dtype(\n            np.float64\n        )\n\n    @staticmethod\n    def _dtype_str(dt):\n        if dt == np.dtype(np.float64):\n            return \"float64\"\n        elif dt == np.dtype(np.float32):\n            return \"float32\"\n        elif dt == np.dtype(np.int64):\n            return \"int64\"\n        elif dt == np.dtype(np.int32):\n            return \"int32\"\n        else:\n            msg = f\"Unsupported dtype '{dt}'\"\n            raise RuntimeError(msg)\n        return None\n\n    # Shapes of decompressed array\n\n    @property\n    def shape(self):\n        \"\"\"The shape of the local, uncompressed array.\"\"\"\n        return self._shape\n\n    @property\n    def global_shape(self):\n        \"\"\"The global shape of array across any MPI communicator.\"\"\"\n        return self._global_shape\n\n    @property\n    def leading_shape(self):\n        \"\"\"The local shape of leading uncompressed dimensions.\"\"\"\n        return self._leading_shape\n\n    @property\n    def global_leading_shape(self):\n        \"\"\"The global shape of leading uncompressed dimensions across all processes.\"\"\"\n        return self._global_leading_shape\n\n    @property\n    def stream_size(self):\n        \"\"\"The uncompressed length of each stream.\"\"\"\n        return self._stream_size\n\n    # Properties of the compressed data\n\n    @property\n    def nbytes(self):\n        \"\"\"The total number of bytes used by compressed data on the local process.\"\"\"\n        return self._local_nbytes\n\n    @property\n    def global_nbytes(self):\n        \"\"\"The sum of total bytes used by compressed data across all processes.\"\"\"\n        return self._global_nbytes\n\n    @property\n    def global_process_nbytes(self):\n        \"\"\"The bytes used by compressed data on each process.\"\"\"\n        return self._global_proc_bytes\n\n    @property\n    def nstreams(self):\n        \"\"\"The number of local streams (product of entries of `leading_shape`)\"\"\"\n        return self._local_nstreams\n\n    @property\n    def global_nstreams(self):\n        \"\"\"Number of global streams (product of entries of `global_leading_shape`)\"\"\"\n        return self._global_nstreams\n\n    @property\n    def compressed(self):\n        \"\"\"The concatenated raw bytes of all streams on the local process.\"\"\"\n        return self._compressed\n\n    @property\n    def stream_starts(self):\n        \"\"\"The array of starting bytes for each stream on the local process.\"\"\"\n        return self._stream_starts\n\n    @property\n    def stream_nbytes(self):\n        \"\"\"The array of nbytes for each stream on the local process.\"\"\"\n        return self._stream_nbytes\n\n    @property\n    def global_stream_starts(self):\n        \"\"\"The array of starting bytes within the global compressed data.\"\"\"\n        return self._global_stream_starts\n\n    @property\n    def global_stream_nbytes(self):\n        \"\"\"The array of nbytes within the global compressed data.\"\"\"\n        return self._global_stream_nbytes\n\n    @property\n    def stream_offsets(self):\n        \"\"\"The value subtracted from each stream during conversion to int32.\"\"\"\n        return self._stream_offsets\n\n    @property\n    def stream_gains(self):\n        \"\"\"The gain factor for each stream during conversion to int32.\"\"\"\n        return self._stream_gains\n\n    @property\n    def mpi_comm(self):\n        \"\"\"The MPI communicator over which the array is distributed.\"\"\"\n        return self._mpi_comm\n\n    @property\n    def mpi_dist(self):\n        \"\"\"The range of the leading dimension assigned to each MPI process.\"\"\"\n        return self._mpi_dist\n\n    @property\n    def dtype(self):\n        \"\"\"The dtype of the uncompressed array.\"\"\"\n        return self._dtype\n\n    @property\n    def typestr(self):\n        \"\"\"A string representation of the original data type.\"\"\"\n        return self._typestr\n\n    # __getitem__ slicing / decompression on the fly and associated\n    # helper functions.\n\n    def _slice_nelem(self, slc, dim):\n        \"\"\"Get the number of elements in a slice.\"\"\"\n        start, stop, step = slc.indices(dim)\n        nslc = (stop - start) // step\n        if nslc &lt; 0:\n            nslc = 0\n        return nslc\n\n    def _keep_view(self, key):\n        \"\"\"Convert leading-shape key to bool array.\"\"\"\n        if len(key) != len(self._leading_shape):\n            msg = f\"keep_view {key} does not match leading \"\n            msg += f\"dimensions {len(self._leading_shape)}\"\n            raise ValueError(msg)\n        view = np.zeros(self._leading_shape, dtype=bool)\n        view[key] = True\n        return view\n\n    def _get_full_key(self, key):\n        \"\"\"Process the incoming key so that it covers all dimensions.\n\n        Args:\n            key (tuple):  The input key consisting of an integer or a tuple\n                of slices and / or integers.\n\n        Result:\n            (tuple):  The full key.\n\n        \"\"\"\n        ndim = len(self._local_shape)\n        full_key = list()\n        if self._flatten_single:\n            # Our array is a single stream with flattened shape.  The user\n            # supplied key should only contain the sample axis.\n            if isinstance(key, tuple):\n                # It better have length == 1...\n                if len(key) != 1:\n                    msg = f\"Slice key {key} is not valid for single, \"\n                    msg += \"flattened stream.\"\n                    raise ValueError(msg)\n                full_key = [0, key[0]]\n            else:\n                # Single element, compress sample dimension\n                full_key = [0, key]\n        else:\n            if isinstance(key, tuple):\n                for axis, axkey in enumerate(key):\n                    full_key.append(axkey)\n            else:\n                full_key.append(key)\n\n        if len(full_key) &gt; ndim:\n            msg = f\"Invalid slice key {key}, too many dimensions\"\n            raise ValueError(msg)\n\n        # Fill in remaining dimensions\n        filled = len(full_key)\n        full_key.extend([slice(None) for x in range(len(self._local_shape) - filled)])\n        return full_key\n\n    def _get_leading_axes(self, full_key):\n        \"\"\"Process the leading axes.\n\n        Args:\n            full_key (tuple):  The full-rank selection key.\n\n        Returns:\n            (tuple):  The (leading_shape, keep array).\n\n        \"\"\"\n        leading_shape = list()\n        keep_slice = list()\n\n        if self._flatten_single:\n            # Our array is a single stream with flattened shape.\n            keep_slice = [0,]\n        else:\n            for axis, axkey in enumerate(full_key[:-1]):\n                if not isinstance(axkey, (int, np.integer)):\n                    # Some kind of slice, do not compress this dimension.\n                    nslc = self._slice_nelem(axkey, self._local_shape[axis])\n                    leading_shape.append(nslc)\n                else:\n                    # Check for validity\n                    if axkey &lt; 0 or axkey &gt;= self._local_shape[axis]:\n                        # Insert a zero-length dimension so that a zero-length\n                        # array is returned in the calling code.\n                        leading_shape.append(0)\n                    else:\n                        # This dimension is a single element and will be\n                        # compressed.\n                        pass\n                keep_slice.append(axkey)\n        leading_shape = tuple(leading_shape)\n        keep_slice = tuple(keep_slice)\n        if len(keep_slice) == 0:\n            keep = None\n        else:\n            keep = self._keep_view(keep_slice)\n        return leading_shape, keep\n\n    def _get_sample_axis(self, full_key):\n        \"\"\"Process any slicing of the stream axis.\n\n        Args:\n            full_key (tuple):  The full-rank selection key.\n\n        Returns:\n            (tuple):  The (first, last, sample_shape).\n\n        \"\"\"\n        sample_key = full_key[-1]\n        if sample_key is None:\n            return (0, self._stream_size, (self._stream_size,))\n        if isinstance(sample_key, slice):\n            start, stop, step = sample_key.indices(self._stream_size)\n            if step != 1:\n                msg = \"Only stride==1 supported on stream slices\"\n                raise ValueError(msg)\n            if stop - start &lt;= 0:\n                # No samples\n                return (0, 0, (0,))\n            return (start, stop, (stop-start,))\n        elif isinstance(sample_key, (int, np.integer)):\n            # Just a scalar\n            return (sample_key, sample_key + 1, ())\n        else:\n            msg = \"Stream dimension supports contiguous slices or single indices.\"\n            raise ValueError(msg)\n\n    def __getitem__(self, raw_key):\n        \"\"\"Decompress a slice of data on the fly.\n\n        Args:\n            raw_key (tuple):  A tuple of slices or integers.\n\n        Returns:\n            (array):  The decompressed array slice.\n\n        \"\"\"\n        # Get the key for all dimensions\n        key = self._get_full_key(raw_key)\n\n        # Compute the output leading shape and keep array\n        leading_shape, keep = self._get_leading_axes(key)\n\n        # Compute sample axis slice\n        first, last, sample_shape = self._get_sample_axis(key)\n\n        full_shape = leading_shape + sample_shape\n        if len(full_shape) == 0:\n            n_total = 0\n        else:\n            n_total = np.prod(full_shape)\n        if n_total == 0:\n            # At least one dimension was zero, return empty array\n            return np.zeros(full_shape, dtype=self._dtype)\n        else:\n            arr, strm_indices = array_decompress_slice(\n                self._compressed,\n                self._stream_size,\n                self._stream_starts,\n                self._stream_nbytes,\n                stream_offsets=self._stream_offsets,\n                stream_gains=self._stream_gains,\n                keep=keep,\n                first_stream_sample=first,\n                last_stream_sample=last,\n                is_int64=self._is_int64,\n            )\n            return arr.reshape(full_shape)\n\n    def __delitem__(self, key):\n        raise RuntimeError(\"Cannot delete individual streams\")\n\n    def __setitem__(self, key, value):\n        raise RuntimeError(\"Cannot modify individual byte streams\")\n\n    def __repr__(self):\n        rank = 0\n        mpistr = \"\"\n        if self._mpi_comm is not None:\n            rank = self._mpi_comm.rank\n            mpistr = f\" | Rank {rank:04d} \"\n            mpistr += f\"{self._mpi_dist[rank][0]}-\"\n            mpistr += f\"{self._mpi_dist[rank][1] - 1} |\"\n        rep = f\"&lt;FlacArray{mpistr} {self._typestr} \"\n        rep += f\"shape={self._shape} bytes={self._local_nbytes}&gt;\"\n        return rep\n\n    def __eq__(self, other):\n        if self._shape != other._shape:\n            log.debug(f\"other shape {other._shape} != {self._shape}\")\n            return False\n        if self._dtype != other._dtype:\n            log.debug(f\"other dtype {other._dtype} != {self._dtype}\")\n            return False\n        if self._global_shape != other._global_shape:\n            msg = f\"other global_shape {other._global_shape} != {self._global_shape}\"\n            log.debug(msg)\n            return False\n        if not np.array_equal(self._stream_starts, other._stream_starts):\n            msg = f\"other starts {other._stream_starts} != {self._stream_starts}\"\n            log.debug(msg)\n            return False\n        if not np.array_equal(self._compressed, other._compressed):\n            msg = f\"other compressed {other._compressed} != {self._compressed}\"\n            log.debug(msg)\n            return False\n        if self._stream_offsets is None:\n            if other._stream_offsets is not None:\n                log.debug(\"other stream_offsets not None, self is None\")\n                return False\n        else:\n            if other._stream_offsets is None:\n                log.debug(\"other stream_offsets is None, self is not None\")\n                return False\n            else:\n                if not np.allclose(self._stream_offsets, other._stream_offsets):\n                    msg = f\"other stream_offsets {other._stream_offsets} != \"\n                    msg += f\"{self._stream_offsets}\"\n                    log.debug(msg)\n                    return False\n        if self._stream_gains is None:\n            if other._stream_gains is not None:\n                log.debug(\"other stream_gains not None, self is None\")\n                return False\n        else:\n            if other._stream_gains is None:\n                log.debug(\"other stream_offsets is None, self is not None\")\n                return False\n            else:\n                if not np.allclose(self._stream_gains, other._stream_gains):\n                    msg = f\"other stream_gains {other._stream_gains} != \"\n                    msg += f\"{self._stream_gains}\"\n                    log.debug(msg)\n                    return False\n        return True\n\n    def to_array(\n        self,\n        keep=None,\n        stream_slice=None,\n        keep_indices=False,\n        use_threads=False,\n    ):\n        \"\"\"Decompress local data into a numpy array.\n\n        This uses the compressed representation to reconstruct a normal numpy\n        array.  The returned data type will be either int32, int64, float32, or\n        float64 depending on the original data type.\n\n        If `stream_slice` is specified, the returned array will have only that\n        range of samples in the final dimension.\n\n        If `keep` is specified, this should be a boolean array with the same shape\n        as the leading dimensions of the original array.  True values in this array\n        indicate that the stream should be kept.\n\n        If `keep` is specified, the returned array WILL NOT have the same shape as\n        the original.  Instead it will be a 2D array of decompressed streams- the\n        streams corresponding to True values in the `keep` mask.\n\n        If `keep_indices` is True and `keep` is specified, then a tuple of two values\n        is returned.  The first is the array of decompressed streams.  The second is\n        a list of tuples, each of which specifies the indices of the stream in the\n        original array.\n\n        Args:\n            keep (array):  Bool array of streams to keep in the decompression.\n            stream_slice (slice):  A python slice with step size of one, indicating\n                the sample range to extract from each stream.\n            keep_indices (bool):  If True, also return the original indices of the\n                streams.\n            use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n                This is only beneficial for large arrays.\n\n        \"\"\"\n        first_samp = None\n        last_samp = None\n        if stream_slice is not None:\n            if stream_slice.step is not None and stream_slice.step != 1:\n                raise RuntimeError(\n                    \"Only stream slices with a step size of 1 are supported\"\n                )\n            first_samp = stream_slice.start\n            last_samp = stream_slice.stop\n\n        arr, indices = array_decompress_slice(\n            self._compressed,\n            self._stream_size,\n            self._stream_starts,\n            self._stream_nbytes,\n            stream_offsets=self._stream_offsets,\n            stream_gains=self._stream_gains,\n            keep=keep,\n            first_stream_sample=first_samp,\n            last_stream_sample=last_samp,\n            is_int64=self._is_int64,\n            use_threads=use_threads,\n            no_flatten=(not self._flatten_single),\n        )\n        if keep is not None and keep_indices:\n            return (arr, indices)\n        else:\n            return arr\n\n    @classmethod\n    def from_array(\n        cls, arr, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False\n    ):\n        \"\"\"Construct a FlacArray from a numpy ndarray.\n\n        Args:\n            arr (numpy.ndarray):  The input array data.\n            level (int):  Compression level (0-8).\n            quanta (float, array):  For floating point data, the floating point\n                increment of each 32bit integer value.  Optionally an iterable of\n                increments, one per stream.\n            precision (int, array):  Number of significant digits to retain in\n                float-to-int conversion.  Alternative to `quanta`.  Optionally an\n                iterable of values, one per stream.\n            mpi_comm (MPI.Comm):  If specified, the input array is assumed to be\n                distributed across the communicator at the leading dimension.  The\n                local piece of the array is passed in on each process.\n            use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n                This is only beneficial for large arrays.\n\n        Returns:\n            (FlacArray):  A newly constructed FlacArray.\n\n        \"\"\"\n        # Get the global shape of the array\n        global_props = global_array_properties(arr.shape, mpi_comm=mpi_comm)\n        global_shape = global_props[\"shape\"]\n        mpi_dist = global_props[\"dist\"]\n\n        # Compress our local piece of the array\n        compressed, starts, nbytes, offsets, gains = array_compress(\n            arr,\n            level=level,\n            quanta=quanta,\n            precision=precision,\n            use_threads=use_threads,\n        )\n\n        return FlacArray(\n            None,\n            shape=arr.shape,\n            global_shape=global_shape,\n            compressed=compressed,\n            dtype=arr.dtype,\n            stream_starts=starts,\n            stream_nbytes=nbytes,\n            stream_offsets=offsets,\n            stream_gains=gains,\n            mpi_comm=mpi_comm,\n            mpi_dist=mpi_dist,\n        )\n\n    def write_hdf5(self, hgrp):\n        \"\"\"Write data to an HDF5 Group.\n\n        The internal object properties are written to an open HDF5 group.  If you\n        wish to use MPI I/O to write data to the group, then you must be using an MPI\n        enabled h5py and you should pass in a valid handle to the group on all\n        processes.\n\n        If the `FlacArray` is distributed over an MPI communicator, but the h5py\n        implementation does not support MPI I/O, then all data will be communicated\n        to the rank zero process for writing.  In this case, the `hgrp` argument should\n        be None except on the root process.\n\n        Args:\n            hgrp (h5py.Group):  The open Group for writing.\n\n        Returns:\n            None\n\n        \"\"\"\n        if self._is_int64:\n            n_channels = 2\n        else:\n            n_channels = 1\n\n        hdf5_write_compressed(\n            hgrp,\n            self._leading_shape,\n            self._global_leading_shape,\n            self._stream_size,\n            self._stream_starts,\n            self._global_stream_starts,\n            self._stream_nbytes,\n            self._stream_offsets,\n            self._stream_gains,\n            self._compressed,\n            n_channels,\n            self._compressed.nbytes,\n            self._global_nbytes,\n            self._global_proc_nbytes,\n            self._mpi_comm,\n            self._mpi_dist,\n        )\n\n    @classmethod\n    def read_hdf5(\n        cls,\n        hgrp,\n        keep=None,\n        mpi_comm=None,\n        mpi_dist=None,\n        no_flatten=False,\n    ):\n        \"\"\"Construct a FlacArray from an HDF5 Group.\n\n        This function loads all information about the array from an HDF5 group.  If\n        `mpi_comm` is specified, the created array is distributed over that\n        communicator.  If you also wish to use MPI I/O to read data from the group,\n        then you must be using an MPI-enabled h5py and you should pass in a valid\n        handle to the group on all processes.\n\n        If `mpi_dist` is specified, it should be an iterable with the number of leading\n        dimension elements assigned to each process.  If None, the leading dimension\n        will be distributed uniformly.\n\n        If `keep` is specified, this should be a boolean array with the same shape\n        as the leading dimensions of the original array.  True values in this array\n        indicate that the stream should be kept.\n\n        If `keep` is specified, the returned array WILL NOT have the same shape as\n        the original.  Instead it will be a 2D array of decompressed streams- the\n        streams corresponding to True values in the `keep` mask.\n\n        Args:\n            hgrp (h5py.Group):  The open Group for reading.\n            keep (array):  Bool array of streams to keep in the decompression.\n            mpi_comm (MPI.Comm):  If specified, the communicator over which to\n                distribute the leading dimension.\n            mpi_dist (array):  If specified, assign blocks of these sizes to processes\n                when distributing the leading dimension.\n            no_flatten (bool):  If True, for single-stream arrays, leave the leading\n                dimension of (1,) in the result.\n\n        Returns:\n            (FlacArray):  A newly constructed FlacArray.\n\n        \"\"\"\n        (\n            local_shape,\n            global_shape,\n            compressed,\n            n_channels,\n            stream_starts,\n            stream_nbytes,\n            stream_offsets,\n            stream_gains,\n            mpi_dist,\n            keep_indices,\n        ) = hdf5_read_compressed(\n            hgrp,\n            keep=keep,\n            mpi_comm=mpi_comm,\n            mpi_dist=mpi_dist,\n        )\n\n        dt = compressed_dtype(n_channels, stream_offsets, stream_gains)\n\n        if (len(local_shape) == 2 and local_shape[0] == 1) and not no_flatten:\n            # Flatten\n            shape = (local_shape[1],)\n        else:\n            shape = local_shape\n\n        return FlacArray(\n            None,\n            shape=shape,\n            global_shape=global_shape,\n            compressed=compressed,\n            dtype=dt,\n            stream_starts=stream_starts,\n            stream_nbytes=stream_nbytes,\n            stream_offsets=stream_offsets,\n            stream_gains=stream_gains,\n            mpi_comm=mpi_comm,\n            mpi_dist=mpi_dist,\n        )\n\n    def write_zarr(self, zgrp):\n        \"\"\"Write data to an Zarr Group.\n\n        The internal object properties are written to an open zarr group.\n\n        If the `FlacArray` is distributed over an MPI communicator, then all data will\n        be communicated to the rank zero process for writing.  In this case, the `zgrp`\n        argument should be None except on the root process.\n\n        Args:\n            zgrp (zarr.Group):  The open Group for writing.\n\n        Returns:\n            None\n\n        \"\"\"\n        if self._is_int64:\n            n_channels = 2\n        else:\n            n_channels = 1\n        zarr_write_compressed(\n            zgrp,\n            self._leading_shape,\n            self._global_leading_shape,\n            self._stream_size,\n            self._stream_starts,\n            self._global_stream_starts,\n            self._stream_nbytes,\n            self._stream_offsets,\n            self._stream_gains,\n            self._compressed,\n            n_channels,\n            self._compressed.nbytes,\n            self._global_nbytes,\n            self._global_proc_nbytes,\n            self._mpi_comm,\n            self._mpi_dist,\n        )\n\n    @classmethod\n    def read_zarr(\n        cls,\n        zgrp,\n        keep=None,\n        mpi_comm=None,\n        mpi_dist=None,\n        no_flatten=False,\n    ):\n        \"\"\"Construct a FlacArray from a Zarr Group.\n\n        This function loads all information about the array from a zarr group.  If\n        `mpi_comm` is specified, the created array is distributed over that\n        communicator.\n\n        If `mpi_dist` is specified, it should be an iterable with the number of leading\n        dimension elements assigned to each process.  If None, the leading dimension\n        will be distributed uniformly.\n\n        If `keep` is specified, this should be a boolean array with the same shape\n        as the leading dimensions of the original array.  True values in this array\n        indicate that the stream should be kept.\n\n        If `keep` is specified, the returned array WILL NOT have the same shape as\n        the original.  Instead it will be a 2D array of decompressed streams- the\n        streams corresponding to True values in the `keep` mask.\n\n        Args:\n            zgrp (zarr.Group):  The open Group for reading.\n            keep (array):  Bool array of streams to keep in the decompression.\n            mpi_comm (MPI.Comm):  If specified, the communicator over which to\n                distribute the leading dimension.\n            mpi_dist (array):  If specified, assign blocks of these sizes to processes\n                when distributing the leading dimension.\n            no_flatten (bool):  If True, for single-stream arrays, leave the leading\n                dimension of (1,) in the result.\n\n        Returns:\n            (FlacArray):  A newly constructed FlacArray.\n\n        \"\"\"\n        (\n            local_shape,\n            global_shape,\n            compressed,\n            n_channels,\n            stream_starts,\n            stream_nbytes,\n            stream_offsets,\n            stream_gains,\n            mpi_dist,\n            keep_indices,\n        ) = zarr_read_compressed(\n            zgrp,\n            keep=keep,\n            mpi_comm=mpi_comm,\n            mpi_dist=mpi_dist,\n        )\n\n        dt = compressed_dtype(n_channels, stream_offsets, stream_gains)\n\n        if (len(local_shape) == 2 and local_shape[0] == 1) and not no_flatten:\n            # Flatten\n            shape = (local_shape[1],)\n        else:\n            shape = local_shape\n\n        return FlacArray(\n            None,\n            shape=shape,\n            global_shape=global_shape,\n            compressed=compressed,\n            dtype=dt,\n            stream_starts=stream_starts,\n            stream_nbytes=stream_nbytes,\n            stream_offsets=stream_offsets,\n            stream_gains=stream_gains,\n            mpi_comm=mpi_comm,\n            mpi_dist=mpi_dist,\n        )\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.compressed","title":"<code>compressed</code>  <code>property</code>","text":"<p>The concatenated raw bytes of all streams on the local process.</p>"},{"location":"reference/#flacarray.FlacArray.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>The dtype of the uncompressed array.</p>"},{"location":"reference/#flacarray.FlacArray.global_leading_shape","title":"<code>global_leading_shape</code>  <code>property</code>","text":"<p>The global shape of leading uncompressed dimensions across all processes.</p>"},{"location":"reference/#flacarray.FlacArray.global_nbytes","title":"<code>global_nbytes</code>  <code>property</code>","text":"<p>The sum of total bytes used by compressed data across all processes.</p>"},{"location":"reference/#flacarray.FlacArray.global_nstreams","title":"<code>global_nstreams</code>  <code>property</code>","text":"<p>Number of global streams (product of entries of <code>global_leading_shape</code>)</p>"},{"location":"reference/#flacarray.FlacArray.global_process_nbytes","title":"<code>global_process_nbytes</code>  <code>property</code>","text":"<p>The bytes used by compressed data on each process.</p>"},{"location":"reference/#flacarray.FlacArray.global_shape","title":"<code>global_shape</code>  <code>property</code>","text":"<p>The global shape of array across any MPI communicator.</p>"},{"location":"reference/#flacarray.FlacArray.global_stream_nbytes","title":"<code>global_stream_nbytes</code>  <code>property</code>","text":"<p>The array of nbytes within the global compressed data.</p>"},{"location":"reference/#flacarray.FlacArray.global_stream_starts","title":"<code>global_stream_starts</code>  <code>property</code>","text":"<p>The array of starting bytes within the global compressed data.</p>"},{"location":"reference/#flacarray.FlacArray.leading_shape","title":"<code>leading_shape</code>  <code>property</code>","text":"<p>The local shape of leading uncompressed dimensions.</p>"},{"location":"reference/#flacarray.FlacArray.mpi_comm","title":"<code>mpi_comm</code>  <code>property</code>","text":"<p>The MPI communicator over which the array is distributed.</p>"},{"location":"reference/#flacarray.FlacArray.mpi_dist","title":"<code>mpi_dist</code>  <code>property</code>","text":"<p>The range of the leading dimension assigned to each MPI process.</p>"},{"location":"reference/#flacarray.FlacArray.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>The total number of bytes used by compressed data on the local process.</p>"},{"location":"reference/#flacarray.FlacArray.nstreams","title":"<code>nstreams</code>  <code>property</code>","text":"<p>The number of local streams (product of entries of <code>leading_shape</code>)</p>"},{"location":"reference/#flacarray.FlacArray.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>The shape of the local, uncompressed array.</p>"},{"location":"reference/#flacarray.FlacArray.stream_gains","title":"<code>stream_gains</code>  <code>property</code>","text":"<p>The gain factor for each stream during conversion to int32.</p>"},{"location":"reference/#flacarray.FlacArray.stream_nbytes","title":"<code>stream_nbytes</code>  <code>property</code>","text":"<p>The array of nbytes for each stream on the local process.</p>"},{"location":"reference/#flacarray.FlacArray.stream_offsets","title":"<code>stream_offsets</code>  <code>property</code>","text":"<p>The value subtracted from each stream during conversion to int32.</p>"},{"location":"reference/#flacarray.FlacArray.stream_size","title":"<code>stream_size</code>  <code>property</code>","text":"<p>The uncompressed length of each stream.</p>"},{"location":"reference/#flacarray.FlacArray.stream_starts","title":"<code>stream_starts</code>  <code>property</code>","text":"<p>The array of starting bytes for each stream on the local process.</p>"},{"location":"reference/#flacarray.FlacArray.typestr","title":"<code>typestr</code>  <code>property</code>","text":"<p>A string representation of the original data type.</p>"},{"location":"reference/#flacarray.FlacArray.__getitem__","title":"<code>__getitem__(raw_key)</code>","text":"<p>Decompress a slice of data on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>raw_key</code> <code>tuple</code> <p>A tuple of slices or integers.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The decompressed array slice.</p> Source code in <code>flacarray/array.py</code> <pre><code>def __getitem__(self, raw_key):\n    \"\"\"Decompress a slice of data on the fly.\n\n    Args:\n        raw_key (tuple):  A tuple of slices or integers.\n\n    Returns:\n        (array):  The decompressed array slice.\n\n    \"\"\"\n    # Get the key for all dimensions\n    key = self._get_full_key(raw_key)\n\n    # Compute the output leading shape and keep array\n    leading_shape, keep = self._get_leading_axes(key)\n\n    # Compute sample axis slice\n    first, last, sample_shape = self._get_sample_axis(key)\n\n    full_shape = leading_shape + sample_shape\n    if len(full_shape) == 0:\n        n_total = 0\n    else:\n        n_total = np.prod(full_shape)\n    if n_total == 0:\n        # At least one dimension was zero, return empty array\n        return np.zeros(full_shape, dtype=self._dtype)\n    else:\n        arr, strm_indices = array_decompress_slice(\n            self._compressed,\n            self._stream_size,\n            self._stream_starts,\n            self._stream_nbytes,\n            stream_offsets=self._stream_offsets,\n            stream_gains=self._stream_gains,\n            keep=keep,\n            first_stream_sample=first,\n            last_stream_sample=last,\n            is_int64=self._is_int64,\n        )\n        return arr.reshape(full_shape)\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.from_array","title":"<code>from_array(arr, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False)</code>  <code>classmethod</code>","text":"<p>Construct a FlacArray from a numpy ndarray.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array data.</p> required <code>level</code> <code>int</code> <p>Compression level (0-8).</p> <code>5</code> <code>quanta</code> <code>(float, array)</code> <p>For floating point data, the floating point increment of each 32bit integer value.  Optionally an iterable of increments, one per stream.</p> <code>None</code> <code>precision</code> <code>(int, array)</code> <p>Number of significant digits to retain in float-to-int conversion.  Alternative to <code>quanta</code>.  Optionally an iterable of values, one per stream.</p> <code>None</code> <code>mpi_comm</code> <code>Comm</code> <p>If specified, the input array is assumed to be distributed across the communicator at the leading dimension.  The local piece of the array is passed in on each process.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlacArray</code> <p>A newly constructed FlacArray.</p> Source code in <code>flacarray/array.py</code> <pre><code>@classmethod\ndef from_array(\n    cls, arr, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False\n):\n    \"\"\"Construct a FlacArray from a numpy ndarray.\n\n    Args:\n        arr (numpy.ndarray):  The input array data.\n        level (int):  Compression level (0-8).\n        quanta (float, array):  For floating point data, the floating point\n            increment of each 32bit integer value.  Optionally an iterable of\n            increments, one per stream.\n        precision (int, array):  Number of significant digits to retain in\n            float-to-int conversion.  Alternative to `quanta`.  Optionally an\n            iterable of values, one per stream.\n        mpi_comm (MPI.Comm):  If specified, the input array is assumed to be\n            distributed across the communicator at the leading dimension.  The\n            local piece of the array is passed in on each process.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    Returns:\n        (FlacArray):  A newly constructed FlacArray.\n\n    \"\"\"\n    # Get the global shape of the array\n    global_props = global_array_properties(arr.shape, mpi_comm=mpi_comm)\n    global_shape = global_props[\"shape\"]\n    mpi_dist = global_props[\"dist\"]\n\n    # Compress our local piece of the array\n    compressed, starts, nbytes, offsets, gains = array_compress(\n        arr,\n        level=level,\n        quanta=quanta,\n        precision=precision,\n        use_threads=use_threads,\n    )\n\n    return FlacArray(\n        None,\n        shape=arr.shape,\n        global_shape=global_shape,\n        compressed=compressed,\n        dtype=arr.dtype,\n        stream_starts=starts,\n        stream_nbytes=nbytes,\n        stream_offsets=offsets,\n        stream_gains=gains,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.read_hdf5","title":"<code>read_hdf5(hgrp, keep=None, mpi_comm=None, mpi_dist=None, no_flatten=False)</code>  <code>classmethod</code>","text":"<p>Construct a FlacArray from an HDF5 Group.</p> <p>This function loads all information about the array from an HDF5 group.  If <code>mpi_comm</code> is specified, the created array is distributed over that communicator.  If you also wish to use MPI I/O to read data from the group, then you must be using an MPI-enabled h5py and you should pass in a valid handle to the group on all processes.</p> <p>If <code>mpi_dist</code> is specified, it should be an iterable with the number of leading dimension elements assigned to each process.  If None, the leading dimension will be distributed uniformly.</p> <p>If <code>keep</code> is specified, this should be a boolean array with the same shape as the leading dimensions of the original array.  True values in this array indicate that the stream should be kept.</p> <p>If <code>keep</code> is specified, the returned array WILL NOT have the same shape as the original.  Instead it will be a 2D array of decompressed streams- the streams corresponding to True values in the <code>keep</code> mask.</p> <p>Parameters:</p> Name Type Description Default <code>hgrp</code> <code>Group</code> <p>The open Group for reading.</p> required <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>mpi_comm</code> <code>Comm</code> <p>If specified, the communicator over which to distribute the leading dimension.</p> <code>None</code> <code>mpi_dist</code> <code>array</code> <p>If specified, assign blocks of these sizes to processes when distributing the leading dimension.</p> <code>None</code> <code>no_flatten</code> <code>bool</code> <p>If True, for single-stream arrays, leave the leading dimension of (1,) in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlacArray</code> <p>A newly constructed FlacArray.</p> Source code in <code>flacarray/array.py</code> <pre><code>@classmethod\ndef read_hdf5(\n    cls,\n    hgrp,\n    keep=None,\n    mpi_comm=None,\n    mpi_dist=None,\n    no_flatten=False,\n):\n    \"\"\"Construct a FlacArray from an HDF5 Group.\n\n    This function loads all information about the array from an HDF5 group.  If\n    `mpi_comm` is specified, the created array is distributed over that\n    communicator.  If you also wish to use MPI I/O to read data from the group,\n    then you must be using an MPI-enabled h5py and you should pass in a valid\n    handle to the group on all processes.\n\n    If `mpi_dist` is specified, it should be an iterable with the number of leading\n    dimension elements assigned to each process.  If None, the leading dimension\n    will be distributed uniformly.\n\n    If `keep` is specified, this should be a boolean array with the same shape\n    as the leading dimensions of the original array.  True values in this array\n    indicate that the stream should be kept.\n\n    If `keep` is specified, the returned array WILL NOT have the same shape as\n    the original.  Instead it will be a 2D array of decompressed streams- the\n    streams corresponding to True values in the `keep` mask.\n\n    Args:\n        hgrp (h5py.Group):  The open Group for reading.\n        keep (array):  Bool array of streams to keep in the decompression.\n        mpi_comm (MPI.Comm):  If specified, the communicator over which to\n            distribute the leading dimension.\n        mpi_dist (array):  If specified, assign blocks of these sizes to processes\n            when distributing the leading dimension.\n        no_flatten (bool):  If True, for single-stream arrays, leave the leading\n            dimension of (1,) in the result.\n\n    Returns:\n        (FlacArray):  A newly constructed FlacArray.\n\n    \"\"\"\n    (\n        local_shape,\n        global_shape,\n        compressed,\n        n_channels,\n        stream_starts,\n        stream_nbytes,\n        stream_offsets,\n        stream_gains,\n        mpi_dist,\n        keep_indices,\n    ) = hdf5_read_compressed(\n        hgrp,\n        keep=keep,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n    )\n\n    dt = compressed_dtype(n_channels, stream_offsets, stream_gains)\n\n    if (len(local_shape) == 2 and local_shape[0] == 1) and not no_flatten:\n        # Flatten\n        shape = (local_shape[1],)\n    else:\n        shape = local_shape\n\n    return FlacArray(\n        None,\n        shape=shape,\n        global_shape=global_shape,\n        compressed=compressed,\n        dtype=dt,\n        stream_starts=stream_starts,\n        stream_nbytes=stream_nbytes,\n        stream_offsets=stream_offsets,\n        stream_gains=stream_gains,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.read_zarr","title":"<code>read_zarr(zgrp, keep=None, mpi_comm=None, mpi_dist=None, no_flatten=False)</code>  <code>classmethod</code>","text":"<p>Construct a FlacArray from a Zarr Group.</p> <p>This function loads all information about the array from a zarr group.  If <code>mpi_comm</code> is specified, the created array is distributed over that communicator.</p> <p>If <code>mpi_dist</code> is specified, it should be an iterable with the number of leading dimension elements assigned to each process.  If None, the leading dimension will be distributed uniformly.</p> <p>If <code>keep</code> is specified, this should be a boolean array with the same shape as the leading dimensions of the original array.  True values in this array indicate that the stream should be kept.</p> <p>If <code>keep</code> is specified, the returned array WILL NOT have the same shape as the original.  Instead it will be a 2D array of decompressed streams- the streams corresponding to True values in the <code>keep</code> mask.</p> <p>Parameters:</p> Name Type Description Default <code>zgrp</code> <code>Group</code> <p>The open Group for reading.</p> required <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>mpi_comm</code> <code>Comm</code> <p>If specified, the communicator over which to distribute the leading dimension.</p> <code>None</code> <code>mpi_dist</code> <code>array</code> <p>If specified, assign blocks of these sizes to processes when distributing the leading dimension.</p> <code>None</code> <code>no_flatten</code> <code>bool</code> <p>If True, for single-stream arrays, leave the leading dimension of (1,) in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlacArray</code> <p>A newly constructed FlacArray.</p> Source code in <code>flacarray/array.py</code> <pre><code>@classmethod\ndef read_zarr(\n    cls,\n    zgrp,\n    keep=None,\n    mpi_comm=None,\n    mpi_dist=None,\n    no_flatten=False,\n):\n    \"\"\"Construct a FlacArray from a Zarr Group.\n\n    This function loads all information about the array from a zarr group.  If\n    `mpi_comm` is specified, the created array is distributed over that\n    communicator.\n\n    If `mpi_dist` is specified, it should be an iterable with the number of leading\n    dimension elements assigned to each process.  If None, the leading dimension\n    will be distributed uniformly.\n\n    If `keep` is specified, this should be a boolean array with the same shape\n    as the leading dimensions of the original array.  True values in this array\n    indicate that the stream should be kept.\n\n    If `keep` is specified, the returned array WILL NOT have the same shape as\n    the original.  Instead it will be a 2D array of decompressed streams- the\n    streams corresponding to True values in the `keep` mask.\n\n    Args:\n        zgrp (zarr.Group):  The open Group for reading.\n        keep (array):  Bool array of streams to keep in the decompression.\n        mpi_comm (MPI.Comm):  If specified, the communicator over which to\n            distribute the leading dimension.\n        mpi_dist (array):  If specified, assign blocks of these sizes to processes\n            when distributing the leading dimension.\n        no_flatten (bool):  If True, for single-stream arrays, leave the leading\n            dimension of (1,) in the result.\n\n    Returns:\n        (FlacArray):  A newly constructed FlacArray.\n\n    \"\"\"\n    (\n        local_shape,\n        global_shape,\n        compressed,\n        n_channels,\n        stream_starts,\n        stream_nbytes,\n        stream_offsets,\n        stream_gains,\n        mpi_dist,\n        keep_indices,\n    ) = zarr_read_compressed(\n        zgrp,\n        keep=keep,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n    )\n\n    dt = compressed_dtype(n_channels, stream_offsets, stream_gains)\n\n    if (len(local_shape) == 2 and local_shape[0] == 1) and not no_flatten:\n        # Flatten\n        shape = (local_shape[1],)\n    else:\n        shape = local_shape\n\n    return FlacArray(\n        None,\n        shape=shape,\n        global_shape=global_shape,\n        compressed=compressed,\n        dtype=dt,\n        stream_starts=stream_starts,\n        stream_nbytes=stream_nbytes,\n        stream_offsets=stream_offsets,\n        stream_gains=stream_gains,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.to_array","title":"<code>to_array(keep=None, stream_slice=None, keep_indices=False, use_threads=False)</code>","text":"<p>Decompress local data into a numpy array.</p> <p>This uses the compressed representation to reconstruct a normal numpy array.  The returned data type will be either int32, int64, float32, or float64 depending on the original data type.</p> <p>If <code>stream_slice</code> is specified, the returned array will have only that range of samples in the final dimension.</p> <p>If <code>keep</code> is specified, this should be a boolean array with the same shape as the leading dimensions of the original array.  True values in this array indicate that the stream should be kept.</p> <p>If <code>keep</code> is specified, the returned array WILL NOT have the same shape as the original.  Instead it will be a 2D array of decompressed streams- the streams corresponding to True values in the <code>keep</code> mask.</p> <p>If <code>keep_indices</code> is True and <code>keep</code> is specified, then a tuple of two values is returned.  The first is the array of decompressed streams.  The second is a list of tuples, each of which specifies the indices of the stream in the original array.</p> <p>Parameters:</p> Name Type Description Default <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>stream_slice</code> <code>slice</code> <p>A python slice with step size of one, indicating the sample range to extract from each stream.</p> <code>None</code> <code>keep_indices</code> <code>bool</code> <p>If True, also return the original indices of the streams.</p> <code>False</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> Source code in <code>flacarray/array.py</code> <pre><code>def to_array(\n    self,\n    keep=None,\n    stream_slice=None,\n    keep_indices=False,\n    use_threads=False,\n):\n    \"\"\"Decompress local data into a numpy array.\n\n    This uses the compressed representation to reconstruct a normal numpy\n    array.  The returned data type will be either int32, int64, float32, or\n    float64 depending on the original data type.\n\n    If `stream_slice` is specified, the returned array will have only that\n    range of samples in the final dimension.\n\n    If `keep` is specified, this should be a boolean array with the same shape\n    as the leading dimensions of the original array.  True values in this array\n    indicate that the stream should be kept.\n\n    If `keep` is specified, the returned array WILL NOT have the same shape as\n    the original.  Instead it will be a 2D array of decompressed streams- the\n    streams corresponding to True values in the `keep` mask.\n\n    If `keep_indices` is True and `keep` is specified, then a tuple of two values\n    is returned.  The first is the array of decompressed streams.  The second is\n    a list of tuples, each of which specifies the indices of the stream in the\n    original array.\n\n    Args:\n        keep (array):  Bool array of streams to keep in the decompression.\n        stream_slice (slice):  A python slice with step size of one, indicating\n            the sample range to extract from each stream.\n        keep_indices (bool):  If True, also return the original indices of the\n            streams.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    \"\"\"\n    first_samp = None\n    last_samp = None\n    if stream_slice is not None:\n        if stream_slice.step is not None and stream_slice.step != 1:\n            raise RuntimeError(\n                \"Only stream slices with a step size of 1 are supported\"\n            )\n        first_samp = stream_slice.start\n        last_samp = stream_slice.stop\n\n    arr, indices = array_decompress_slice(\n        self._compressed,\n        self._stream_size,\n        self._stream_starts,\n        self._stream_nbytes,\n        stream_offsets=self._stream_offsets,\n        stream_gains=self._stream_gains,\n        keep=keep,\n        first_stream_sample=first_samp,\n        last_stream_sample=last_samp,\n        is_int64=self._is_int64,\n        use_threads=use_threads,\n        no_flatten=(not self._flatten_single),\n    )\n    if keep is not None and keep_indices:\n        return (arr, indices)\n    else:\n        return arr\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.write_hdf5","title":"<code>write_hdf5(hgrp)</code>","text":"<p>Write data to an HDF5 Group.</p> <p>The internal object properties are written to an open HDF5 group.  If you wish to use MPI I/O to write data to the group, then you must be using an MPI enabled h5py and you should pass in a valid handle to the group on all processes.</p> <p>If the <code>FlacArray</code> is distributed over an MPI communicator, but the h5py implementation does not support MPI I/O, then all data will be communicated to the rank zero process for writing.  In this case, the <code>hgrp</code> argument should be None except on the root process.</p> <p>Parameters:</p> Name Type Description Default <code>hgrp</code> <code>Group</code> <p>The open Group for writing.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>flacarray/array.py</code> <pre><code>def write_hdf5(self, hgrp):\n    \"\"\"Write data to an HDF5 Group.\n\n    The internal object properties are written to an open HDF5 group.  If you\n    wish to use MPI I/O to write data to the group, then you must be using an MPI\n    enabled h5py and you should pass in a valid handle to the group on all\n    processes.\n\n    If the `FlacArray` is distributed over an MPI communicator, but the h5py\n    implementation does not support MPI I/O, then all data will be communicated\n    to the rank zero process for writing.  In this case, the `hgrp` argument should\n    be None except on the root process.\n\n    Args:\n        hgrp (h5py.Group):  The open Group for writing.\n\n    Returns:\n        None\n\n    \"\"\"\n    if self._is_int64:\n        n_channels = 2\n    else:\n        n_channels = 1\n\n    hdf5_write_compressed(\n        hgrp,\n        self._leading_shape,\n        self._global_leading_shape,\n        self._stream_size,\n        self._stream_starts,\n        self._global_stream_starts,\n        self._stream_nbytes,\n        self._stream_offsets,\n        self._stream_gains,\n        self._compressed,\n        n_channels,\n        self._compressed.nbytes,\n        self._global_nbytes,\n        self._global_proc_nbytes,\n        self._mpi_comm,\n        self._mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.FlacArray.write_zarr","title":"<code>write_zarr(zgrp)</code>","text":"<p>Write data to an Zarr Group.</p> <p>The internal object properties are written to an open zarr group.</p> <p>If the <code>FlacArray</code> is distributed over an MPI communicator, then all data will be communicated to the rank zero process for writing.  In this case, the <code>zgrp</code> argument should be None except on the root process.</p> <p>Parameters:</p> Name Type Description Default <code>zgrp</code> <code>Group</code> <p>The open Group for writing.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>flacarray/array.py</code> <pre><code>def write_zarr(self, zgrp):\n    \"\"\"Write data to an Zarr Group.\n\n    The internal object properties are written to an open zarr group.\n\n    If the `FlacArray` is distributed over an MPI communicator, then all data will\n    be communicated to the rank zero process for writing.  In this case, the `zgrp`\n    argument should be None except on the root process.\n\n    Args:\n        zgrp (zarr.Group):  The open Group for writing.\n\n    Returns:\n        None\n\n    \"\"\"\n    if self._is_int64:\n        n_channels = 2\n    else:\n        n_channels = 1\n    zarr_write_compressed(\n        zgrp,\n        self._leading_shape,\n        self._global_leading_shape,\n        self._stream_size,\n        self._stream_starts,\n        self._global_stream_starts,\n        self._stream_nbytes,\n        self._stream_offsets,\n        self._stream_gains,\n        self._compressed,\n        n_channels,\n        self._compressed.nbytes,\n        self._global_nbytes,\n        self._global_proc_nbytes,\n        self._mpi_comm,\n        self._mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#direct-io","title":"Direct I/O","text":"<p>Sometimes code has no need to store compressed arrays in memory. Instead, it may be desirable to have full arrays in memory and compressed arrays on disk. In those situations, you can use several helper functions to write and read numpy arrays directly to / from files.</p>"},{"location":"reference/#hdf5","title":"HDF5","text":"<p>You can write to / read from an h5py Group using functions in the <code>hdf5</code> submodule.</p>"},{"location":"reference/#flacarray.hdf5.write_array","title":"<code>flacarray.hdf5.write_array(arr, hgrp, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False)</code>","text":"<p>Compress a numpy array and write to an HDF5 group.</p> <p>This function is useful if you do not need to access the compressed array in memory and only wish to write it directly to HDF5.  The input array is compressed and then the <code>write_compressed()</code> function is called.</p> <p>If the input array is int32 or int64, the compression is lossless and the compressed bytes and ancillary data is written to datasets within the output group.  If the array is float32 or float64, either the <code>quanta</code> or <code>precision</code> must be specified. See discussion in the <code>FlacArray</code> class documentation about how the offsets and gains are computed for a given quanta.  The offsets and gains are also written as datasets within the output group.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>array</code> <p>The input numpy array.</p> required <code>hgrp</code> <code>Group</code> <p>The Group to use.</p> required <code>level</code> <code>int</code> <p>Compression level (0-8).</p> <code>5</code> <code>quanta</code> <code>(float, array)</code> <p>For floating point data, the floating point increment of each 32bit integer value.  Optionally an iterable of increments, one per stream.</p> <code>None</code> <code>precision</code> <code>(int, array)</code> <p>Number of significant digits to retain in float-to-int conversion.  Alternative to <code>quanta</code>.  Optionally an iterable of values, one per stream.</p> <code>None</code> <code>mpi_comm</code> <code>Comm</code> <p>If specified, the input array is assumed to be distributed across the communicator at the leading dimension.  The local piece of the array is passed in on each process.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>flacarray/hdf5.py</code> <pre><code>@function_timer\ndef write_array(\n    arr, hgrp, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False\n):\n    \"\"\"Compress a numpy array and write to an HDF5 group.\n\n    This function is useful if you do not need to access the compressed array in memory\n    and only wish to write it directly to HDF5.  The input array is compressed and then\n    the `write_compressed()` function is called.\n\n    If the input array is int32 or int64, the compression is lossless and the compressed\n    bytes and ancillary data is written to datasets within the output group.  If the\n    array is float32 or float64, either the `quanta` or `precision` must be specified.\n    See discussion in the `FlacArray` class documentation about how the offsets and\n    gains are computed for a given quanta.  The offsets and gains are also written as\n    datasets within the output group.\n\n    Args:\n        arr (array):  The input numpy array.\n        hgrp (h5py.Group):  The Group to use.\n        level (int):  Compression level (0-8).\n        quanta (float, array):  For floating point data, the floating point\n            increment of each 32bit integer value.  Optionally an iterable of\n            increments, one per stream.\n        precision (int, array):  Number of significant digits to retain in\n            float-to-int conversion.  Alternative to `quanta`.  Optionally an\n            iterable of values, one per stream.\n        mpi_comm (MPI.Comm):  If specified, the input array is assumed to be\n            distributed across the communicator at the leading dimension.  The\n            local piece of the array is passed in on each process.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    Returns:\n        None\n\n    \"\"\"\n    if not have_hdf5:\n        raise RuntimeError(\"h5py is not importable, cannot write to HDF5\")\n\n    # Get the global shape of the array\n    global_props = global_array_properties(arr.shape, mpi_comm=mpi_comm)\n    global_shape = global_props[\"shape\"]\n    mpi_dist = global_props[\"dist\"]\n\n    # Get the number of channels\n    if arr.dtype == np.dtype(np.int64) or arr.dtype == np.dtype(np.float64):\n        n_channels = 2\n    else:\n        n_channels = 1\n\n    # Compress our local piece of the array\n    compressed, starts, nbytes, offsets, gains = array_compress(\n        arr, level=level, quanta=quanta, precision=precision, use_threads=use_threads\n    )\n\n    local_nbytes = compressed.nbytes\n    global_nbytes, global_proc_bytes, global_starts = global_bytes(\n        local_nbytes, starts, mpi_comm\n    )\n    stream_size = arr.shape[-1]\n\n    if len(arr.shape) == 1:\n        leading_shape = (1,)\n    else:\n        leading_shape = arr.shape[:-1]\n    global_leading_shape = global_shape[:-1]\n\n    write_compressed(\n        hgrp,\n        leading_shape,\n        global_leading_shape,\n        stream_size,\n        starts,\n        global_starts,\n        nbytes,\n        offsets,\n        gains,\n        compressed,\n        n_channels,\n        local_nbytes,\n        global_nbytes,\n        global_proc_bytes,\n        mpi_comm,\n        mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.hdf5.read_array","title":"<code>flacarray.hdf5.read_array(hgrp, keep=None, stream_slice=None, keep_indices=False, mpi_comm=None, mpi_dist=None, use_threads=False)</code>","text":"<p>Load a numpy array from compressed HDF5.</p> <p>This function is useful if you do not need to store a compressed representation of the array in memory.  Each stream will be read individually from the file and the desired slice decompressed.  This avoids storing the full compressed data.</p> <p>This function acts as a dispatch to the correct version of the reading function. The function is selected based on the format version string in the data.</p> <p>If <code>stream_slice</code> is specified, the returned array will have only that range of samples in the final dimension.</p> <p>If <code>keep</code> is specified, this should be a boolean array with the same shape as the leading dimensions of the original array.  True values in this array indicate that the stream should be kept.</p> <p>If <code>keep</code> is specified, the returned array WILL NOT have the same shape as the original.  Instead it will be a 2D array of decompressed streams- the streams corresponding to True values in the <code>keep</code> mask.</p> <p>If <code>keep_indices</code> is True and <code>keep</code> is specified, then an additional list is returned containing the indices of each stream that was kept.</p> <p>Parameters:</p> Name Type Description Default <code>hgrp</code> <code>Group</code> <p>The group to read.</p> required <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>stream_slice</code> <code>slice</code> <p>A python slice with step size of one, indicating the sample range to extract from each stream.</p> <code>None</code> <code>keep_indices</code> <code>bool</code> <p>If True, also return the original indices of the streams.</p> <code>False</code> <code>mpi_comm</code> <code>Comm</code> <p>The optional MPI communicator over which to distribute the leading dimension of the array.</p> <code>None</code> <code>mpi_dist</code> <code>list</code> <p>The optional list of tuples specifying the first / last element of the leading dimension to assign to each process.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>The loaded and decompressed data OR the array and the kept indices.</p> Source code in <code>flacarray/hdf5.py</code> <pre><code>@function_timer\ndef read_array(\n    hgrp,\n    keep=None,\n    stream_slice=None,\n    keep_indices=False,\n    mpi_comm=None,\n    mpi_dist=None,\n    use_threads=False,\n):\n    \"\"\"Load a numpy array from compressed HDF5.\n\n    This function is useful if you do not need to store a compressed representation\n    of the array in memory.  Each stream will be read individually from the file and\n    the desired slice decompressed.  This avoids storing the full compressed data.\n\n    This function acts as a dispatch to the correct version of the reading function.\n    The function is selected based on the format version string in the data.\n\n    If `stream_slice` is specified, the returned array will have only that\n    range of samples in the final dimension.\n\n    If `keep` is specified, this should be a boolean array with the same shape\n    as the leading dimensions of the original array.  True values in this array\n    indicate that the stream should be kept.\n\n    If `keep` is specified, the returned array WILL NOT have the same shape as\n    the original.  Instead it will be a 2D array of decompressed streams- the\n    streams corresponding to True values in the `keep` mask.\n\n    If `keep_indices` is True and `keep` is specified, then an additional list\n    is returned containing the indices of each stream that was kept.\n\n    Args:\n        hgrp (h5py.Group):  The group to read.\n        keep (array):  Bool array of streams to keep in the decompression.\n        stream_slice (slice):  A python slice with step size of one, indicating\n            the sample range to extract from each stream.\n        keep_indices (bool):  If True, also return the original indices of the\n            streams.\n        mpi_comm (MPI.Comm):  The optional MPI communicator over which to distribute\n            the leading dimension of the array.\n        mpi_dist (list):  The optional list of tuples specifying the first / last\n            element of the leading dimension to assign to each process.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    Returns:\n        (array):  The loaded and decompressed data OR the array and the kept indices.\n\n    \"\"\"\n    if not have_hdf5:\n        raise RuntimeError(\"h5py is not importable, cannot write to HDF5\")\n\n    format_version = None\n    if hgrp is not None:\n        if \"flacarray_format_version\" in hgrp.attrs:\n            format_version = hgrp.attrs[\"flacarray_format_version\"]\n    if mpi_comm is not None:\n        format_version = mpi_comm.bcast(format_version, root=0)\n    if format_version is None:\n        raise RuntimeError(\"h5py Group does not contain a FlacArray\")\n\n    mod_name = f\".hdf5_load_v{format_version}\"\n    mod = importlib.import_module(mod_name, package=\"flacarray\")\n    read_func = getattr(mod, \"read_array\")\n    return read_func(\n        hgrp,\n        keep=keep,\n        stream_slice=stream_slice,\n        keep_indices=keep_indices,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n        use_threads=use_threads,\n        no_flatten=False,\n    )\n</code></pre>"},{"location":"reference/#zarr","title":"Zarr","text":"<p>You can write to / read from a zarr hierarch Group using functions in the <code>zarr</code> submodule.</p>"},{"location":"reference/#flacarray.zarr.write_array","title":"<code>flacarray.zarr.write_array(arr, zgrp, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False)</code>","text":"<p>Compress a numpy array and write to an Zarr group.</p> <p>This function is useful if you do not need to access the compressed array in memory and only wish to write it directly to Zarr files.  The input array is compressed and then the <code>write_compressed()</code> function is called.</p> <p>If the input array is int32 or int64, the compression is lossless and the compressed bytes and ancillary data is written to datasets within the output group.  If the array is float32 or float64, either the <code>quanta</code> or <code>precision</code> must be specified. See discussion in the <code>FlacArray</code> class documentation about how the offsets and gains are computed for a given quanta.  The offsets and gains are also written as datasets within the output group.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>array</code> <p>The input numpy array.</p> required <code>zgrp</code> <code>Group</code> <p>The Group to use.</p> required <code>level</code> <code>int</code> <p>Compression level (0-8).</p> <code>5</code> <code>quanta</code> <code>(float, array)</code> <p>For floating point data, the floating point increment of each 32bit integer value.  Optionally an iterable of increments, one per stream.</p> <code>None</code> <code>precision</code> <code>(int, array)</code> <p>Number of significant digits to retain in float-to-int conversion.  Alternative to <code>quanta</code>.  Optionally an iterable of values, one per stream.</p> <code>None</code> <code>mpi_comm</code> <code>Comm</code> <p>If specified, the input array is assumed to be distributed across the communicator at the leading dimension.  The local piece of the array is passed in on each process.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>flacarray/zarr.py</code> <pre><code>@function_timer\ndef write_array(\n    arr, zgrp, level=5, quanta=None, precision=None, mpi_comm=None, use_threads=False\n):\n    \"\"\"Compress a numpy array and write to an Zarr group.\n\n    This function is useful if you do not need to access the compressed array in memory\n    and only wish to write it directly to Zarr files.  The input array is compressed\n    and then the `write_compressed()` function is called.\n\n    If the input array is int32 or int64, the compression is lossless and the compressed\n    bytes and ancillary data is written to datasets within the output group.  If the\n    array is float32 or float64, either the `quanta` or `precision` must be specified.\n    See discussion in the `FlacArray` class documentation about how the offsets and\n    gains are computed for a given quanta.  The offsets and gains are also written as\n    datasets within the output group.\n\n    Args:\n        arr (array):  The input numpy array.\n        zgrp (zarr.Group):  The Group to use.\n        level (int):  Compression level (0-8).\n        quanta (float, array):  For floating point data, the floating point\n            increment of each 32bit integer value.  Optionally an iterable of\n            increments, one per stream.\n        precision (int, array):  Number of significant digits to retain in\n            float-to-int conversion.  Alternative to `quanta`.  Optionally an\n            iterable of values, one per stream.\n        mpi_comm (MPI.Comm):  If specified, the input array is assumed to be\n            distributed across the communicator at the leading dimension.  The\n            local piece of the array is passed in on each process.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    Returns:\n        None\n\n    \"\"\"\n    if not have_zarr:\n        raise RuntimeError(\"zarr is not importable, cannot write to zarr.Group\")\n\n    # Get the global shape of the array\n    global_props = global_array_properties(arr.shape, mpi_comm=mpi_comm)\n    global_shape = global_props[\"shape\"]\n    mpi_dist = global_props[\"dist\"]\n\n    # Get the number of channels\n    if arr.dtype == np.dtype(np.int64) or arr.dtype == np.dtype(np.float64):\n        n_channels = 2\n    else:\n        n_channels = 1\n\n    # Compress our local piece of the array\n    compressed, starts, nbytes, offsets, gains = array_compress(\n        arr, level=level, quanta=quanta, precision=precision, use_threads=use_threads\n    )\n\n    local_nbytes = compressed.nbytes\n    global_nbytes, global_proc_bytes, global_starts = global_bytes(\n        local_nbytes, starts, mpi_comm\n    )\n    stream_size = arr.shape[-1]\n\n    if len(arr.shape) == 1:\n        leading_shape = (1,)\n    else:\n        leading_shape = arr.shape[:-1]\n    global_leading_shape = global_shape[:-1]\n\n    write_compressed(\n        zgrp,\n        leading_shape,\n        global_leading_shape,\n        stream_size,\n        starts,\n        global_starts,\n        nbytes,\n        offsets,\n        gains,\n        compressed,\n        n_channels,\n        local_nbytes,\n        global_nbytes,\n        global_proc_bytes,\n        mpi_comm,\n        mpi_dist,\n    )\n</code></pre>"},{"location":"reference/#flacarray.zarr.read_array","title":"<code>flacarray.zarr.read_array(zgrp, keep=None, stream_slice=None, keep_indices=False, mpi_comm=None, mpi_dist=None, use_threads=False, no_flatten=False)</code>","text":"<p>Load a numpy array from a compressed Zarr group.</p> <p>This function is useful if you do not need to store a compressed representation of the array in memory.  Each stream will be read individually from the file and the desired slice decompressed.  This avoids storing the full compressed data.</p> <p>This function acts as a dispatch to the correct version of the reading function. The function is selected based on the format version string in the data.</p> <p>If <code>stream_slice</code> is specified, the returned array will have only that range of samples in the final dimension.</p> <p>If <code>keep</code> is specified, this should be a boolean array with the same shape as the leading dimensions of the original array.  True values in this array indicate that the stream should be kept.</p> <p>If <code>keep</code> is specified, the returned array WILL NOT have the same shape as the original.  Instead it will be a 2D array of decompressed streams- the streams corresponding to True values in the <code>keep</code> mask.</p> <p>If <code>keep_indices</code> is True and <code>keep</code> is specified, then an additional list is returned containing the indices of each stream that was kept.</p> <p>Parameters:</p> Name Type Description Default <code>zgrp</code> <code>Group</code> <p>The group to read.</p> required <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>stream_slice</code> <code>slice</code> <p>A python slice with step size of one, indicating the sample range to extract from each stream.</p> <code>None</code> <code>keep_indices</code> <code>bool</code> <p>If True, also return the original indices of the streams.</p> <code>False</code> <code>mpi_comm</code> <code>Comm</code> <p>The optional MPI communicator over which to distribute the leading dimension of the array.</p> <code>None</code> <code>mpi_dist</code> <code>list</code> <p>The optional list of tuples specifying the first / last element of the leading dimension to assign to each process.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <code>no_flatten</code> <code>bool</code> <p>If True, for single-stream arrays, leave the leading dimension of (1,) in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>The loaded and decompressed data OR the array and the kept indices.</p> Source code in <code>flacarray/zarr.py</code> <pre><code>@function_timer\ndef read_array(\n    zgrp,\n    keep=None,\n    stream_slice=None,\n    keep_indices=False,\n    mpi_comm=None,\n    mpi_dist=None,\n    use_threads=False,\n    no_flatten=False,\n):\n    \"\"\"Load a numpy array from a compressed Zarr group.\n\n    This function is useful if you do not need to store a compressed representation\n    of the array in memory.  Each stream will be read individually from the file and\n    the desired slice decompressed.  This avoids storing the full compressed data.\n\n    This function acts as a dispatch to the correct version of the reading function.\n    The function is selected based on the format version string in the data.\n\n    If `stream_slice` is specified, the returned array will have only that\n    range of samples in the final dimension.\n\n    If `keep` is specified, this should be a boolean array with the same shape\n    as the leading dimensions of the original array.  True values in this array\n    indicate that the stream should be kept.\n\n    If `keep` is specified, the returned array WILL NOT have the same shape as\n    the original.  Instead it will be a 2D array of decompressed streams- the\n    streams corresponding to True values in the `keep` mask.\n\n    If `keep_indices` is True and `keep` is specified, then an additional list\n    is returned containing the indices of each stream that was kept.\n\n    Args:\n        zgrp (zarr.Group):  The group to read.\n        keep (array):  Bool array of streams to keep in the decompression.\n        stream_slice (slice):  A python slice with step size of one, indicating\n            the sample range to extract from each stream.\n        keep_indices (bool):  If True, also return the original indices of the\n            streams.\n        mpi_comm (MPI.Comm):  The optional MPI communicator over which to distribute\n            the leading dimension of the array.\n        mpi_dist (list):  The optional list of tuples specifying the first / last\n            element of the leading dimension to assign to each process.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n        no_flatten (bool):  If True, for single-stream arrays, leave the leading\n            dimension of (1,) in the result.\n\n    Returns:\n        (array):  The loaded and decompressed data OR the array and the kept indices.\n\n    \"\"\"\n    if not have_zarr:\n        raise RuntimeError(\"zarr is not importable, cannot write to a Zarr Group\")\n\n    format_version = None\n    if zgrp is not None:\n        if \"flacarray_format_version\" in zgrp.attrs:\n            format_version = zgrp.attrs[\"flacarray_format_version\"]\n    if mpi_comm is not None:\n        format_version = mpi_comm.bcast(format_version, root=0)\n    if format_version is None:\n        raise RuntimeError(\"Zarr Group does not contain a FlacArray\")\n\n    mod_name = f\".zarr_load_v{format_version}\"\n    mod = importlib.import_module(mod_name, package=\"flacarray\")\n    read_func = getattr(mod, \"read_array\")\n    return read_func(\n        zgrp,\n        keep=keep,\n        stream_slice=stream_slice,\n        keep_indices=keep_indices,\n        mpi_comm=mpi_comm,\n        mpi_dist=mpi_dist,\n        use_threads=use_threads,\n        no_flatten=False,\n    )\n</code></pre>"},{"location":"reference/#interactive-tools","title":"Interactive Tools","text":"<p>The <code>flacarray.demo</code> submodule contains a few helper functions that are not imported by default. You will need to have optional dependencies (matplotlib) installed to use the visualization tools. For testing, it is convenient to generate arrays consisting of random timestreams with some structure. The <code>create_fake_data</code> function can be used for this.</p> <p>Most data arrays in practice have 2 or 3 dimensions. If the number of streams is relatively small, then an uncompressed array can be plotted with the <code>plot_data</code> function.</p>"},{"location":"reference/#flacarray.demo.create_fake_data","title":"<code>flacarray.demo.create_fake_data(local_shape, sigma=1.0, dtype=np.float64, seed=123456789, comm=None, dc_sigma=5)</code>","text":"<p>Create fake random data for testing.</p> <p>This is a helper function to generate some random data for testing. if <code>sigma</code> is None, uniform randoms are return.  If sigma is not None, samples drawn from a Gaussian distribution are returned.</p> <p>If <code>comm</code> is not None, the data is created on one process and then pieces are distributed among the processes.</p> <p>Parameters:</p> Name Type Description Default <code>local_shape</code> <code>tuple</code> <p>The local shape of the data on this process.</p> required <code>sigma</code> <code>float</code> <p>The width of the distribution or None.</p> <code>1.0</code> <code>dtype</code> <code>dtype</code> <p>The data type of the returned array.</p> <code>float64</code> <code>seed</code> <code>int</code> <p>The optional seed for np.random.</p> <code>123456789</code> <code>comm</code> <code>Comm</code> <p>The MPI communicator or None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(The random data on the local process, MPI distribution).</p> Source code in <code>flacarray/demo.py</code> <pre><code>def create_fake_data(\n    local_shape, sigma=1.0, dtype=np.float64, seed=123456789, comm=None, dc_sigma=5,\n):\n    \"\"\"Create fake random data for testing.\n\n    This is a helper function to generate some random data for testing.\n    if `sigma` is None, uniform randoms are return.  If sigma is not None,\n    samples drawn from a Gaussian distribution are returned.\n\n    If `comm` is not None, the data is created on one process and then pieces are\n    distributed among the processes.\n\n    Args:\n        local_shape (tuple):  The local shape of the data on this process.\n        sigma (float):  The width of the distribution or None.\n        dtype (np.dtype):  The data type of the returned array.\n        seed (int):  The optional seed for np.random.\n        comm (MPI.Comm):  The MPI communicator or None.\n\n    Returns:\n        (tuple):  (The random data on the local process, MPI distribution).\n\n    \"\"\"\n    if comm is None:\n        rank = 0\n    else:\n        rank = comm.rank\n\n    # Get the global array properties\n    gprops = global_array_properties(local_shape, comm)\n    shape = gprops[\"shape\"]\n    mpi_dist = gprops[\"dist\"]\n\n    flatshape = np.prod(shape)\n    stream_size = shape[-1]\n    leading_shape = shape[:-1]\n    leading_shape_ext = leading_shape + (1,)\n\n    rng = np.random.default_rng(seed=seed)\n    global_data = None\n    if rank == 0:\n        if sigma is None:\n            # Uniform randoms. Verify that we can fully encode the high / low\n            # values by setting a few samples to those extremes.\n            if dtype == np.dtype(np.int64) or dtype == np.dtype(np.int32):\n                low = np.iinfo(dtype).min\n                high = np.iinfo(dtype).max\n                flat_data = rng.integers(\n                    low=low, high=high, size=flatshape, dtype=np.int64\n                ).astype(dtype)\n            else:\n                low = np.finfo(dtype).min\n                high = np.finfo(dtype).max\n                flat_data = rng.uniform(\n                    low=low, high=high, size=flatshape, dtype=np.float64\n                ).astype(dtype)\n            flat_data[0] = low\n            flat_data[1] = high\n            global_data = flat_data.reshape(shape)\n        else:\n            # Construct a random DC level for each stream.\n            if dc_sigma is None:\n                dc = 0\n            else:\n                dc = dc_sigma * sigma * (rng.random(size=leading_shape_ext) - 0.5)\n\n            # Construct a simple low frequency waveform (assume 1Hz sampling)\n            wave = np.zeros(stream_size, dtype=dtype)\n            t = np.arange(stream_size)\n            minf = 5 / stream_size\n            for freq, amp in zip([3 * minf, minf], [2 * sigma, 6 * sigma]):\n                wave[:] += amp * np.sin(2 * np.pi * freq * t)\n\n            # Initialize all streams to a scaled version of this waveform plus\n            # the DC level\n            scale = rng.random(size=leading_shape_ext)\n            global_data = np.empty(shape, dtype=dtype)\n            if len(leading_shape) == 0:\n                global_data[:] = dc\n                global_data[:] += scale * wave\n            else:\n                leading_slc = tuple([slice(None) for x in leading_shape])\n                global_data[leading_slc] = dc\n                global_data[leading_slc] += scale * wave\n\n            # Add some Gaussian random noise to each stream\n            global_data[:] += rng.normal(0.0, sigma, flatshape).reshape(shape)\n    if comm is not None:\n        global_data = comm.bcast(global_data, root=0)\n\n    # Extract our local piece of the global data\n    if len(leading_shape) == 0 or (len(leading_shape) == 1 and leading_shape[0] == 1):\n        data = global_data\n    else:\n        local_start = mpi_dist[rank][0]\n        local_stop = mpi_dist[rank][1]\n        local_slice = [slice(local_start, local_stop, 1)]\n        local_slice.extend([slice(None) for x in shape[1:]])\n        local_slice = tuple(local_slice)\n        data = global_data[local_slice]\n    if len(data.shape) == 2 and data.shape[0] == 1:\n        data = data.reshape((-1))\n\n    return data, mpi_dist\n</code></pre>"},{"location":"reference/#flacarray.demo.plot_data","title":"<code>flacarray.demo.plot_data(data, keep=None, stream_slc=slice(None), file=None)</code>","text":"Source code in <code>flacarray/demo.py</code> <pre><code>def plot_data(data, keep=None, stream_slc=slice(None), file=None):\n    # We only import matplotlib if we are actually going to make some plots.\n    # This is not a required package.\n    import matplotlib.pyplot as plt\n\n    if len(data.shape) &gt; 3:\n        raise NotImplementedError(\"Can only plot 1D and 2D arrays of streams\")\n\n    if len(data.shape) == 1:\n        plot_rows = 1\n        plot_cols = 1\n    elif len(data.shape) == 2:\n        plot_rows = data.shape[0]\n        plot_cols = 1\n    else:\n        plot_rows = data.shape[1]\n        plot_cols = data.shape[0]\n\n    fig_dpi = 100\n    fig_width = 6 * plot_cols\n    fig_height = 4 * plot_rows\n    fig = plt.figure(figsize=(fig_width, fig_height), dpi=fig_dpi)\n    if len(data.shape) == 1:\n        # Single stream\n        ax = fig.add_subplot(1, 1, 1, aspect=\"auto\")\n        ax.plot(data[stream_slc])\n    elif len(data.shape) == 2:\n        # 1-D array of streams, plot vertically\n        for iplot in range(data.shape[0]):\n            ax = fig.add_subplot(plot_rows, 1, iplot + 1, aspect=\"auto\")\n            ax.plot(data[iplot, stream_slc])\n    else:\n        # 2-D array of streams, plot in a grid\n        for row in range(plot_rows):\n            for col in range(plot_cols):\n                slc = (col, row, stream_slc)\n                ax = fig.add_subplot(\n                    plot_rows, plot_cols, row * plot_cols + col + 1, aspect=\"auto\"\n                )\n                ax.plot(data[slc], color=\"black\")\n    if file is None:\n        plt.show()\n    else:\n        plt.savefig(file)\n        plt.close()\n</code></pre>"},{"location":"reference/#low-level-tools","title":"Low-Level Tools","text":"<p>For specialized use cases, you can also work directly with the compressed bytestream and auxiliary arrays and convert to / from numpy arrays.</p>"},{"location":"reference/#flacarray.compress.array_compress","title":"<code>flacarray.compress.array_compress(arr, level=5, quanta=None, precision=None, use_threads=False)</code>","text":"<p>Compress a numpy array with optional floating point conversion.</p> <p>If <code>arr</code> is an int32 array, the returned stream offsets and gains will be None. if <code>arr</code> is an int64 array, the returned stream offsets and gains will be None and the calling code is responsible for tracking that the compressed bytes are associated with a 64bit stream.</p> <p>If the input array is float32 or float64, exactly one of quanta or precision must be specified.  Both float32 and float64 data will have floating point offset and gain arrays returned.  See discussion in the <code>FlacArray</code> class documentation about how the offsets and gains are computed for a given quanta.</p> <p>The shape of the returned auxiliary arrays (starts, nbytes, etc) will have a shape corresponding to the leading shape of the input array.  If the input array is a single stream, the returned auxiliary information will be arrays with a single element.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The input array data.</p> required <code>level</code> <code>int</code> <p>Compression level (0-8).</p> <code>5</code> <code>quanta</code> <code>(float, array)</code> <p>For floating point data, the floating point increment of each integer value.  Optionally an array of increments, one per stream.</p> <code>None</code> <code>precision</code> <code>(int, array)</code> <p>Number of significant digits to retain in float-to-int conversion.  Alternative to <code>quanta</code>.  Optionally an iterable of values, one per stream.</p> <code>None</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>The (compressed bytes, stream starts, stream_nbytes, stream offsets, stream gains)</p> Source code in <code>flacarray/compress.py</code> <pre><code>@function_timer\ndef array_compress(arr, level=5, quanta=None, precision=None, use_threads=False):\n    \"\"\"Compress a numpy array with optional floating point conversion.\n\n    If `arr` is an int32 array, the returned stream offsets and gains will be None.\n    if `arr` is an int64 array, the returned stream offsets and gains will be None and\n    the calling code is responsible for tracking that the compressed bytes are\n    associated with a 64bit stream.\n\n    If the input array is float32 or float64, exactly one of quanta or precision\n    must be specified.  Both float32 and float64 data will have floating point offset\n    and gain arrays returned.  See discussion in the `FlacArray` class documentation\n    about how the offsets and gains are computed for a given quanta.\n\n    The shape of the returned auxiliary arrays (starts, nbytes, etc) will have a shape\n    corresponding to the leading shape of the input array.  If the input array is a\n    single stream, the returned auxiliary information will be arrays with a single\n    element.\n\n    Args:\n        arr (numpy.ndarray):  The input array data.\n        level (int):  Compression level (0-8).\n        quanta (float, array):  For floating point data, the floating point\n            increment of each integer value.  Optionally an array of increments,\n            one per stream.\n        precision (int, array):  Number of significant digits to retain in\n            float-to-int conversion.  Alternative to `quanta`.  Optionally an\n            iterable of values, one per stream.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n\n    Returns:\n        (tuple): The (compressed bytes, stream starts, stream_nbytes, stream offsets,\n            stream gains)\n\n    \"\"\"\n    if arr.size == 0:\n        raise ValueError(\"Cannot compress a zero-sized array!\")\n    leading_shape = arr.shape[:-1]\n\n    if arr.dtype == np.dtype(np.float32) or arr.dtype == np.dtype(np.float64):\n        # Floating point data\n        if quanta is None and precision is None:\n            msg = f\"Compressing floating point data ('{arr.dtype}') \"\n            msg = \"requires specifying either quanta or precision.\"\n            raise RuntimeError(msg)\n        if quanta is not None:\n            if precision is not None:\n                raise RuntimeError(\"Cannot set both quanta and precision\")\n            try:\n                nq = len(quanta)\n                # This is an array\n                if nq.shape != leading_shape:\n                    msg = \"If not a scalar, quanta must have the same shape as the \"\n                    msg += \"leading dimensions of the array\"\n                    raise ValueError(msg)\n                dquanta = quanta.astype(arr.dtype)\n            except TypeError:\n                # This is a scalar, applied to all detectors\n                dquanta = quanta * np.ones(leading_shape, dtype=arr.dtype)\n        else:\n            # We are using precision instead\n            dquanta = None\n        idata, foff, gains = float_to_int(arr, quanta=dquanta, precision=precision)\n        (compressed, starts, nbytes) = encode_flac(\n            idata, level, use_threads=use_threads\n        )\n        return (compressed, starts, nbytes, foff, gains)\n    elif arr.dtype == np.dtype(np.int32) or arr.dtype == np.dtype(np.int64):\n        # Integer data\n        (compressed, starts, nbytes) = encode_flac(arr, level, use_threads=use_threads)\n        return (compressed, starts, nbytes, None, None)\n    else:\n        raise ValueError(f\"Unsupported data type '{arr.dtype}'\")\n</code></pre>"},{"location":"reference/#flacarray.decompress.array_decompress","title":"<code>flacarray.decompress.array_decompress(compressed, stream_size, stream_starts, stream_nbytes, stream_offsets=None, stream_gains=None, first_stream_sample=None, last_stream_sample=None, is_int64=False, use_threads=False, no_flatten=False)</code>","text":"<p>Decompress a FLAC encoded array and restore original data type.</p> <p>If both <code>stream_gains</code> and <code>stream_offsets</code> are specified, the output will be floating point data.  If neither is specified, the output will be integer data. It is an error to specify only one of those options.</p> <p>The compressed byte stream might contain either int32 or int64 data, and the calling code is responsible for tracking this.  The <code>is_int64</code> parameter should be set to True if the byte stream contains 64bit integers.</p> <p>To decompress a subset of samples in all streams, specify the <code>first_stream_sample</code> and <code>last_stream_sample</code> values.  None values or negative values disable this feature.</p> <p>Parameters:</p> Name Type Description Default <code>compressed</code> <code>array</code> <p>The array of compressed bytes.</p> required <code>stream_size</code> <code>int</code> <p>The length of the decompressed final dimension.</p> required <code>stream_starts</code> <code>array</code> <p>The array of starting bytes in the bytestream.</p> required <code>stream_nbytes</code> <code>array</code> <p>The array of number of bytes in each stream.</p> required <code>stream_offsets</code> <code>array</code> <p>The array of offsets, one per stream.</p> <code>None</code> <code>stream_gains</code> <code>array</code> <p>The array of gains, one per stream.</p> <code>None</code> <code>first_stream_sample</code> <code>int</code> <p>The first sample of every stream to decompress.</p> <code>None</code> <code>last_stream_sample</code> <code>int</code> <p>The last sample of every stream to decompress.</p> <code>None</code> <code>is_int64</code> <code>bool</code> <p>If True, the compressed stream contains 64bit integers.</p> <code>False</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <code>no_flatten</code> <code>bool</code> <p>If True, for single-stream arrays, leave the leading dimension of (1,) in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>array</code> <p>The output array.</p> Source code in <code>flacarray/decompress.py</code> <pre><code>@function_timer\ndef array_decompress(\n    compressed,\n    stream_size,\n    stream_starts,\n    stream_nbytes,\n    stream_offsets=None,\n    stream_gains=None,\n    first_stream_sample=None,\n    last_stream_sample=None,\n    is_int64=False,\n    use_threads=False,\n    no_flatten=False,\n):\n    \"\"\"Decompress a FLAC encoded array and restore original data type.\n\n    If both `stream_gains` and `stream_offsets` are specified, the output will be\n    floating point data.  If neither is specified, the output will be integer data.\n    It is an error to specify only one of those options.\n\n    The compressed byte stream might contain either int32 or int64 data, and the calling\n    code is responsible for tracking this.  The `is_int64` parameter should be set to\n    True if the byte stream contains 64bit integers.\n\n    To decompress a subset of samples in all streams, specify the `first_stream_sample`\n    and `last_stream_sample` values.  None values or negative values disable this\n    feature.\n\n    Args:\n        compressed (array):  The array of compressed bytes.\n        stream_size (int):  The length of the decompressed final dimension.\n        stream_starts (array):  The array of starting bytes in the bytestream.\n        stream_nbytes (array):  The array of number of bytes in each stream.\n        stream_offsets (array):  The array of offsets, one per stream.\n        stream_gains (array):  The array of gains, one per stream.\n        first_stream_sample (int):  The first sample of every stream to decompress.\n        last_stream_sample (int):  The last sample of every stream to decompress.\n        is_int64 (bool):  If True, the compressed stream contains 64bit integers.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n        no_flatten (bool):  If True, for single-stream arrays, leave the leading\n            dimension of (1,) in the result.\n\n    Returns:\n        (array): The output array.\n\n    \"\"\"\n    arr, _ = array_decompress_slice(\n        compressed,\n        stream_size,\n        stream_starts,\n        stream_nbytes,\n        stream_offsets=stream_offsets,\n        stream_gains=stream_gains,\n        keep=None,\n        first_stream_sample=first_stream_sample,\n        last_stream_sample=last_stream_sample,\n        is_int64=is_int64,\n        use_threads=use_threads,\n        no_flatten=no_flatten,\n    )\n    return arr\n</code></pre>"},{"location":"reference/#flacarray.decompress.array_decompress_slice","title":"<code>flacarray.decompress.array_decompress_slice(compressed, stream_size, stream_starts, stream_nbytes, stream_offsets=None, stream_gains=None, keep=None, first_stream_sample=None, last_stream_sample=None, is_int64=False, use_threads=False, no_flatten=False)</code>","text":"<p>Decompress a slice of a FLAC encoded array and restore original data type.</p> <p>If both <code>stream_gains</code> and <code>stream_offsets</code> are specified, the output will be floating point data.  If neither is specified, the output will be integer data. It is an error to specify only one of those options.</p> <p>The compressed byte stream might contain either int32 or int64 data, and the calling code is responsible for tracking this.  The <code>is_int64</code> parameter should be set to True if the byte stream contains 64bit integers.</p> <p>To decompress a subset of samples in all streams, specify the <code>first_stream_sample</code> and <code>last_stream_sample</code> values.  None values or negative values disable this feature.</p> <p>To decompress a subset of streams, pass a boolean array to the <code>keep</code> argument. This should have the same shape as the <code>starts</code> array.  Only streams with a True value in the <code>keep</code> array will be decompressed.</p> <p>If the <code>keep</code> array is specified, the output tuple will contain the 2D array of streams that were kept, as well as a list of tuples indicating the original array indices for each stream in the output.  If the <code>keep</code> array is None, the output tuple will contain an array with the original N-dimensional leading array shape and the trailing number of samples.  The second element of the tuple will be None.</p> <p>Parameters:</p> Name Type Description Default <code>compressed</code> <code>array</code> <p>The array of compressed bytes.</p> required <code>stream_size</code> <code>int</code> <p>The length of the decompressed final dimension.</p> required <code>stream_starts</code> <code>array</code> <p>The array of starting bytes in the bytestream.</p> required <code>stream_nbytes</code> <code>array</code> <p>The array of number of bytes in each stream.</p> required <code>stream_offsets</code> <code>array</code> <p>The array of offsets, one per stream.</p> <code>None</code> <code>stream_gains</code> <code>array</code> <p>The array of gains, one per stream.</p> <code>None</code> <code>keep</code> <code>array</code> <p>Bool array of streams to keep in the decompression.</p> <code>None</code> <code>first_stream_sample</code> <code>int</code> <p>The first sample of every stream to decompress.</p> <code>None</code> <code>last_stream_sample</code> <code>int</code> <p>The last sample of every stream to decompress.</p> <code>None</code> <code>is_int64</code> <code>bool</code> <p>If True, the compressed stream contains 64bit integers.</p> <code>False</code> <code>use_threads</code> <code>bool</code> <p>If True, use OpenMP threads to parallelize decoding. This is only beneficial for large arrays.</p> <code>False</code> <code>no_flatten</code> <code>bool</code> <p>If True, for single-stream arrays, leave the leading dimension of (1,) in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>The (output array, list of stream indices).</p> Source code in <code>flacarray/decompress.py</code> <pre><code>@function_timer\ndef array_decompress_slice(\n    compressed,\n    stream_size,\n    stream_starts,\n    stream_nbytes,\n    stream_offsets=None,\n    stream_gains=None,\n    keep=None,\n    first_stream_sample=None,\n    last_stream_sample=None,\n    is_int64=False,\n    use_threads=False,\n    no_flatten=False,\n):\n    \"\"\"Decompress a slice of a FLAC encoded array and restore original data type.\n\n    If both `stream_gains` and `stream_offsets` are specified, the output will be\n    floating point data.  If neither is specified, the output will be integer data.\n    It is an error to specify only one of those options.\n\n    The compressed byte stream might contain either int32 or int64 data, and the calling\n    code is responsible for tracking this.  The `is_int64` parameter should be set to\n    True if the byte stream contains 64bit integers.\n\n    To decompress a subset of samples in all streams, specify the `first_stream_sample`\n    and `last_stream_sample` values.  None values or negative values disable this\n    feature.\n\n    To decompress a subset of streams, pass a boolean array to the `keep` argument.\n    This should have the same shape as the `starts` array.  Only streams with a True\n    value in the `keep` array will be decompressed.\n\n    If the `keep` array is specified, the output tuple will contain the 2D array of\n    streams that were kept, as well as a list of tuples indicating the original array\n    indices for each stream in the output.  If the `keep` array is None, the output\n    tuple will contain an array with the original N-dimensional leading array shape\n    and the trailing number of samples.  The second element of the tuple will be None.\n\n    Args:\n        compressed (array):  The array of compressed bytes.\n        stream_size (int):  The length of the decompressed final dimension.\n        stream_starts (array):  The array of starting bytes in the bytestream.\n        stream_nbytes (array):  The array of number of bytes in each stream.\n        stream_offsets (array):  The array of offsets, one per stream.\n        stream_gains (array):  The array of gains, one per stream.\n        keep (array):  Bool array of streams to keep in the decompression.\n        first_stream_sample (int):  The first sample of every stream to decompress.\n        last_stream_sample (int):  The last sample of every stream to decompress.\n        is_int64 (bool):  If True, the compressed stream contains 64bit integers.\n        use_threads (bool):  If True, use OpenMP threads to parallelize decoding.\n            This is only beneficial for large arrays.\n        no_flatten (bool):  If True, for single-stream arrays, leave the leading\n            dimension of (1,) in the result.\n\n    Returns:\n        (tuple): The (output array, list of stream indices).\n\n    \"\"\"\n    if first_stream_sample is None:\n        first_stream_sample = -1\n    if last_stream_sample is None:\n        last_stream_sample = -1\n\n    # If we have one stream, ensure that our auxiliary data are arrays\n    is_scalar = False\n    if (\n        not isinstance(stream_starts, np.ndarray) or\n        (len(stream_starts.shape) == 1 and stream_starts.shape[0] == 1)\n    ):\n        # This is a scalar\n        is_scalar = True\n        stream_starts = ensure_one_element(stream_starts, np.int64)\n        stream_nbytes = ensure_one_element(stream_nbytes, np.int64)\n        if stream_offsets is not None:\n            # We have float values\n            if is_int64:\n                stream_offsets = ensure_one_element(stream_offsets, np.float64)\n                stream_gains = ensure_one_element(stream_gains, np.float64)\n            else:\n                stream_offsets = ensure_one_element(stream_offsets, np.float32)\n                stream_gains = ensure_one_element(stream_gains, np.float32)\n\n    starts, nbytes, indices = keep_select(keep, stream_starts, stream_nbytes)\n    offsets = select_keep_indices(stream_offsets, indices)\n    gains = select_keep_indices(stream_gains, indices)\n\n    if stream_offsets is not None:\n        if stream_gains is not None:\n            # This is floating point data.\n            idata = decode_flac(\n                compressed,\n                starts,\n                nbytes,\n                stream_size,\n                first_sample=first_stream_sample,\n                last_sample=last_stream_sample,\n                use_threads=use_threads,\n                is_int64=is_int64,\n            )\n            arr = int_to_float(idata, offsets, gains)\n        else:\n            raise RuntimeError(\n                \"When specifying offsets, you must also provide the gains\"\n            )\n    else:\n        if stream_gains is not None:\n            raise RuntimeError(\n                \"When specifying gains, you must also provide the offsets\"\n            )\n        # This is integer data\n        arr = decode_flac(\n            compressed,\n            starts,\n            nbytes,\n            stream_size,\n            first_sample=first_stream_sample,\n            last_sample=last_stream_sample,\n            use_threads=use_threads,\n            is_int64=is_int64,\n        )\n    if is_scalar and not no_flatten:\n        return (arr.reshape((-1)), indices)\n    else:\n        return (arr, indices)\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport h5py # For optional I/O operations\nimport zarr # For optional I/O operations\n</pre> import numpy as np import h5py # For optional I/O operations import zarr # For optional I/O operations In\u00a0[2]: Copied! <pre>from flacarray import FlacArray, demo\n</pre> from flacarray import FlacArray, demo In\u00a0[3]: Copied! <pre># Create a 3D array where the last dimension is the \"streams\" we are compressing.\narr, _ = demo.create_fake_data((4, 3, 10000))\n# How large is this in memory?\nprint(f\"Input array is {arr.nbytes} bytes\")\n</pre> # Create a 3D array where the last dimension is the \"streams\" we are compressing. arr, _ = demo.create_fake_data((4, 3, 10000)) # How large is this in memory? print(f\"Input array is {arr.nbytes} bytes\") <pre>Input array is 960000 bytes\n</pre> In\u00a0[4]: Copied! <pre># Plot these streams\ndemo.plot_data(arr)\n</pre> # Plot these streams demo.plot_data(arr) In\u00a0[5]: Copied! <pre># Create a compressed array\nflcarr = FlacArray.from_array(arr, precision=10)\n</pre> # Create a compressed array flcarr = FlacArray.from_array(arr, precision=10) In\u00a0[6]: Copied! <pre># Properties of the compressed array\nprint(flcarr)\n</pre> # Properties of the compressed array print(flcarr) <pre>&lt;FlacArray float64 shape=(4, 3, 10000) bytes=522899&gt;\n</pre> In\u00a0[7]: Copied! <pre># Restore back to an array\nrestored = flcarr.to_array()\ndemo.plot_data(restored)\n</pre> # Restore back to an array restored = flcarr.to_array() demo.plot_data(restored) In\u00a0[8]: Copied! <pre># Plot the residual\nresidual = restored - arr\ndemo.plot_data(residual)\n</pre> # Plot the residual residual = restored - arr demo.plot_data(residual) In\u00a0[9]: Copied! <pre>subarr = flcarr[1:2, :, 200:300]\ndemo.plot_data(subarr)\n</pre> subarr = flcarr[1:2, :, 200:300] demo.plot_data(subarr) In\u00a0[10]: Copied! <pre>with h5py.File(\"flcarr.h5\", \"w\") as hf:\n    flcarr.write_hdf5(hf)\n</pre> with h5py.File(\"flcarr.h5\", \"w\") as hf:     flcarr.write_hdf5(hf) In\u00a0[11]: Copied! <pre># We can load this back into a new FlacArray using a class method\nwith h5py.File(\"flcarr.h5\", \"r\") as hf:\n    new_flcarr = FlacArray.read_hdf5(hf)\n</pre> # We can load this back into a new FlacArray using a class method with h5py.File(\"flcarr.h5\", \"r\") as hf:     new_flcarr = FlacArray.read_hdf5(hf) In\u00a0[12]: Copied! <pre># The compressed representations should be equal...\nprint(new_flcarr == flcarr)\n</pre> # The compressed representations should be equal... print(new_flcarr == flcarr) <pre>True\n</pre> <p>You can also load in just a subset of the streams using a \"keep\" mask.  This is a boolean array with the same shape as the leading dimensions of the original array.</p> In\u00a0[13]: Copied! <pre>leading_shape = arr.shape[:-1]\nkeep = np.zeros(leading_shape, dtype=bool)\n# Select the first and last stream on the second row\nkeep[1, 0] = True\nkeep[1, -1] = True\n</pre> leading_shape = arr.shape[:-1] keep = np.zeros(leading_shape, dtype=bool) # Select the first and last stream on the second row keep[1, 0] = True keep[1, -1] = True In\u00a0[14]: Copied! <pre># Load just these streams\nwith h5py.File(\"flcarr.h5\", \"r\") as hf:\n    sub_flcarr = FlacArray.read_hdf5(hf, keep=keep)\n</pre> # Load just these streams with h5py.File(\"flcarr.h5\", \"r\") as hf:     sub_flcarr = FlacArray.read_hdf5(hf, keep=keep) In\u00a0[15]: Copied! <pre># Decompress and plot\ndemo.plot_data(sub_flcarr.to_array())\n</pre> # Decompress and plot demo.plot_data(sub_flcarr.to_array()) In\u00a0[16]: Copied! <pre>import flacarray.hdf5\n</pre> import flacarray.hdf5 In\u00a0[17]: Copied! <pre># Write a numpy array directly to HDF5.  This is equivalent to doing:\n#\n# temp = FlacArray.from_array(arr)\n# with h5py.File(\"test.h5\", \"w\") as hf:\n#     temp.write_hdf5(hf)\n#\nwith h5py.File(\"test.h5\", \"w\") as hf:\n    flacarray.hdf5.write_array(arr, hf, precision=10)\n</pre> # Write a numpy array directly to HDF5.  This is equivalent to doing: # # temp = FlacArray.from_array(arr) # with h5py.File(\"test.h5\", \"w\") as hf: #     temp.write_hdf5(hf) # with h5py.File(\"test.h5\", \"w\") as hf:     flacarray.hdf5.write_array(arr, hf, precision=10) In\u00a0[18]: Copied! <pre>with h5py.File(\"test.h5\", \"r\") as hf:\n    restored = flacarray.hdf5.read_array(hf)\n</pre> with h5py.File(\"test.h5\", \"r\") as hf:     restored = flacarray.hdf5.read_array(hf) In\u00a0[19]: Copied! <pre>demo.plot_data(restored)\n</pre> demo.plot_data(restored) In\u00a0[20]: Copied! <pre># Load only a subset of streams and a slice of samples in those streams.\n# This is equivalent to the following code:\n#\n# with h5py.File(\"test.h5\", \"r\") as hf:\n#    restored = FlacArray.read_hdf5(hf, keep=keep)\n#    sub_restored = restored[:, 200:300]\n#\nwith h5py.File(\"test.h5\", \"r\") as hf:\n    sub_restored = flacarray.hdf5.read_array(hf, keep=keep, stream_slice=slice(200, 300, 1))\n</pre> # Load only a subset of streams and a slice of samples in those streams. # This is equivalent to the following code: # # with h5py.File(\"test.h5\", \"r\") as hf: #    restored = FlacArray.read_hdf5(hf, keep=keep) #    sub_restored = restored[:, 200:300] # with h5py.File(\"test.h5\", \"r\") as hf:     sub_restored = flacarray.hdf5.read_array(hf, keep=keep, stream_slice=slice(200, 300, 1)) In\u00a0[21]: Copied! <pre>demo.plot_data(sub_restored)\n</pre> demo.plot_data(sub_restored) In\u00a0[22]: Copied! <pre>import flacarray.zarr\n</pre> import flacarray.zarr In\u00a0[23]: Copied! <pre>with flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"w\") as zf:\n    flacarray.zarr.write_array(arr, zf, precision=10)\n</pre> with flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"w\") as zf:     flacarray.zarr.write_array(arr, zf, precision=10) In\u00a0[24]: Copied! <pre>with flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"r\") as zf:\n    restored = flacarray.zarr.read_array(zf)\n</pre> with flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"r\") as zf:     restored = flacarray.zarr.read_array(zf) In\u00a0[25]: Copied! <pre>demo.plot_data(restored)\n</pre> demo.plot_data(restored) In\u00a0[26]: Copied! <pre># Specifying a keep mask and sample slice also works.\nwith flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"r\") as zf:\n    sub_restored = flacarray.zarr.read_array(zf, keep=keep, stream_slice=slice(200, 300, 1))\n</pre> # Specifying a keep mask and sample slice also works. with flacarray.zarr.ZarrGroup(\"test.zarr\", mode=\"r\") as zf:     sub_restored = flacarray.zarr.read_array(zf, keep=keep, stream_slice=slice(200, 300, 1)) In\u00a0[27]: Copied! <pre>demo.plot_data(sub_restored)\n</pre> demo.plot_data(sub_restored)"},{"location":"tutorial/#tutorial","title":"Tutorial\u00b6","text":"<p>The <code>flacarray</code> package has tools for working with compressed arrays in memory, as well as saving and loading those to several file formats.  This tutorial makes use of some interactive helper functions in the <code>flacarray.demo</code> package.</p>"},{"location":"tutorial/#flacarray-compressed-arrays-in-memory","title":"<code>FlacArray</code> - Compressed Arrays in Memory\u00b6","text":"<p>The primary class for working with compressed arrays in memory is the <code>FlacArray</code> class.  You can construct one of these from a numpy array with a class method.  First create some fake data in a numpy array for testing.  This is a small 3-D array and the final dimension is always the one that is compressed.  This last dimension should consist of \"streams\" of data.</p>"},{"location":"tutorial/#create-from-array","title":"Create From Array\u00b6","text":"<p>Now create a <code>FlacArray</code> from this.  Integer data is compressed in a lossless fashion, but since this is floating point data, we must choose either a quantization value or a fixed precision (number of decimal places) when converting to integers.</p>"},{"location":"tutorial/#decompress-back-to-array","title":"Decompress Back to Array\u00b6","text":"<p>Now decompress back to a numpy array.  The results will only be bitwise identical for arrays consisting of 32 bit integers.</p>"},{"location":"tutorial/#slicing","title":"Slicing\u00b6","text":"<p>A subset of the full array can be decompressed on the fly.  Any fancy array indexing can be used for the leading dimensions, but only contiguous slices or individual samples are supported in the last dimension.</p>"},{"location":"tutorial/#writing-and-reading","title":"Writing and Reading\u00b6","text":"<p>The <code>FlacArray</code> class has methods to write the internal compressed data and metadata to both h5py and zarr groups.  The data members written to these file formats are simple arrays and scalars.  Supporting other formats in the future would be straightforward.  When decompressing data from disk, you can choose to decompress only a subset of the streams.  Here is an example writing the compressed array to HDF5 and loading it back in.</p>"},{"location":"tutorial/#direct-io-and-compression-of-numpy-arrays","title":"Direct I/O and Compression of Numpy Arrays\u00b6","text":"<p>For some use cases, there is no need to keep the full compressed data in memory (in a <code>FlacArray</code>).  Instead, a normal numpy array is compressed when writing to a file and decompressed back into a numpy array when reading.  The package has high-level functions for performing this kind of operation.  When decompressing, a subset of streams can be loaded from disk, and then a sample range can be specified when doing the decompression.</p>"},{"location":"tutorial/#hdf5","title":"HDF5\u00b6","text":"<p>The <code>hdf5</code> sub-module has helper functions for direct I/O to HDF5.</p>"},{"location":"tutorial/#zarr","title":"Zarr\u00b6","text":"<p>The zarr package provides an h5py-like interface for creating groups with attributes and \"datasets\" (arrays) on disk.  Given an existing <code>zarr.hierarchy.Group</code>, you can compress and write an array and then load it back in.  This is almost identical to the HDF5 syntax above.  The flacarray package includes a helper class (<code>ZarrGroup</code>) which acts as a context manager around an open file.  However if you already have a handle to <code>Group</code> you can pass that directly to <code>flacarray.zarr.write_array()</code> and <code>flacarray.zarr.read_array()</code>.</p>"}]}